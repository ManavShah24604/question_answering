{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6830f326",
   "metadata": {
    "papermill": {
     "duration": 0.016773,
     "end_time": "2024-04-27T16:28:23.333988",
     "exception": false,
     "start_time": "2024-04-27T16:28:23.317215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc08c21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:23.368867Z",
     "iopub.status.busy": "2024-04-27T16:28:23.368470Z",
     "iopub.status.idle": "2024-04-27T16:28:33.718886Z",
     "shell.execute_reply": "2024-04-27T16:28:33.717891Z"
    },
    "papermill": {
     "duration": 10.37132,
     "end_time": "2024-04-27T16:28:33.721594",
     "exception": false,
     "start_time": "2024-04-27T16:28:23.350274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re, os, string, typing, gc, json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def load_json(path):\n",
    "    '''\n",
    "    Loads the JSON file of the Squad dataset.\n",
    "    Returns the json object of the dataset.\n",
    "    '''\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())\n",
    "    print(\"Title: \", data['data'][0]['title'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_data(data:dict)->list:\n",
    "    '''\n",
    "    Parses the JSON file of Squad dataset by looping through the\n",
    "    keys and values and returns a list of dictionaries with\n",
    "    context, query and label triplets being the keys of each dict.\n",
    "    '''\n",
    "    data = data['data']\n",
    "    qa_list = []\n",
    "\n",
    "    for paragraphs in data:\n",
    "\n",
    "        for para in paragraphs['paragraphs']:\n",
    "            context = para['context']\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                \n",
    "                id = qa['id']\n",
    "                question = qa['question']\n",
    "                \n",
    "                for ans in qa['answers']:\n",
    "                    answer = ans['text']\n",
    "                    ans_start = ans['answer_start']\n",
    "                    ans_end = ans_start + len(answer)\n",
    "                    \n",
    "                    qa_dict = {}\n",
    "                    qa_dict['id'] = id\n",
    "                    qa_dict['context'] = context\n",
    "                    qa_dict['question'] = question\n",
    "                    qa_dict['label'] = [ans_start, ans_end]\n",
    "\n",
    "                    qa_dict['answer'] = answer\n",
    "                    qa_list.append(qa_dict)    \n",
    "\n",
    "    \n",
    "    return qa_list\n",
    "\n",
    "\n",
    "\n",
    "def filter_large_examples(df):\n",
    "    '''\n",
    "    Returns ids of examples where context lengths, query lengths and answer lengths are\n",
    "    above a particular threshold. These ids can then be dropped from the dataframe. \n",
    "    This is explicitly mentioned in QANet but can be done for other models as well.\n",
    "    '''\n",
    "    \n",
    "    ctx_lens = []\n",
    "    query_lens = []\n",
    "    ans_lens = []\n",
    "    for index, row in df.iterrows():\n",
    "        ctx_tokens = [w.text for w in nlp(row.context, disable=['parser','ner','tagger'])]\n",
    "        if len(ctx_tokens)>400:\n",
    "            ctx_lens.append(row.name)\n",
    "\n",
    "        query_tokens = [w.text for w in nlp(row.question, disable=['parser','tagger','ner'])]\n",
    "        if len(query_tokens)>50:\n",
    "            query_lens.append(row.name)\n",
    "\n",
    "        ans_tokens = [w.text for w in nlp(row.answer, disable=['parser','tagger','ner'])]\n",
    "        if len(ans_tokens)>30:\n",
    "            ans_lens.append(row.name)\n",
    "\n",
    "        assert row.name == index\n",
    "    \n",
    "    return set(ans_lens + ctx_lens + query_lens)\n",
    "\n",
    "\n",
    "def gather_text_for_vocab(dfs:list):\n",
    "    '''\n",
    "    Gathers text from contexts and questions to build a vocabulary.\n",
    "    \n",
    "    :param dfs: list of dataframes of SQUAD dataset.\n",
    "    :returns: list of contexts and questions\n",
    "    '''\n",
    "    \n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "    \n",
    "    assert len(text) == total\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_word_vocab(vocab_text):\n",
    "    '''\n",
    "    Builds a word-level vocabulary from the given text.\n",
    "    \n",
    "    :param list vocab_text: list of contexts and questions\n",
    "    :returns \n",
    "        dict word2idx: word to index mapping of words\n",
    "        dict idx2word: integer to word mapping\n",
    "        list word_vocab: list of words sorted by frequency\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser','tagger','ner']):\n",
    "            words.append(word.text)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    \n",
    "    \n",
    "    return word2idx, idx2word, word_vocab\n",
    "\n",
    "\n",
    "def build_char_vocab(vocab_text):\n",
    "    '''\n",
    "    Builds a character-level vocabulary from the given text.\n",
    "    \n",
    "    :param list vocab_text: list of contexts and questions\n",
    "    :returns \n",
    "        dict char2idx: character to index mapping of words\n",
    "        list char_vocab: list of characters sorted by frequency\n",
    "    '''\n",
    "    \n",
    "    chars = []\n",
    "    for sent in vocab_text:\n",
    "        for ch in sent:\n",
    "            chars.append(ch)\n",
    "\n",
    "    char_counter = Counter(chars)\n",
    "    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\n",
    "    print(f\"raw-char-vocab: {len(char_vocab)}\")\n",
    "    high_freq_char = [char for char, count in char_counter.items() if count>=20]\n",
    "    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\n",
    "    print(f\"char-vocab-intersect: {len(char_vocab)}\")\n",
    "    char_vocab.insert(0,'<unk>')\n",
    "    char_vocab.insert(1,'<pad>')\n",
    "    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\n",
    "    print(f\"char2idx-length: {len(char2idx)}\")\n",
    "    \n",
    "    return char2idx, char_vocab\n",
    "\n",
    "\n",
    "\n",
    "def context_to_ids(text, word2idx):\n",
    "    '''\n",
    "    Converts context text to their respective ids by mapping each word\n",
    "    using word2idx. Input text is tokenized using spacy tokenizer first.\n",
    "    \n",
    "    :param str text: context text to be converted\n",
    "    :param dict word2idx: word to id mapping\n",
    "\n",
    "    :returns list context_ids: list of mapped ids\n",
    "    \n",
    "    :raises assertion error: sanity check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    context_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n",
    "    context_ids = [word2idx[word] for word in context_tokens]\n",
    "    \n",
    "    assert len(context_ids) == len(context_tokens)\n",
    "    return context_ids\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def question_to_ids(text, word2idx):\n",
    "    '''\n",
    "    Converts question text to their respective ids by mapping each word\n",
    "    using word2idx. Input text is tokenized using spacy tokenizer first.\n",
    "    \n",
    "    :param str text: question text to be converted\n",
    "    :param dict word2idx: word to id mapping\n",
    "    :returns list context_ids: list of mapped ids\n",
    "    \n",
    "    :raises assertion error: sanity check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    question_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n",
    "    question_ids = [word2idx[word] for word in question_tokens]\n",
    "    \n",
    "    assert len(question_ids) == len(question_tokens)\n",
    "    return question_ids\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def test_indices(df, idx2word):\n",
    "    '''\n",
    "    Performs the tests mentioned above. This method also gets the start and end of the answers\n",
    "    with respect to the context_ids for each example.\n",
    "    \n",
    "    :param dataframe df: SQUAD df\n",
    "    :param dict idx2word: inverse mapping of token ids to words\n",
    "    :returns\n",
    "        list start_value_error: example idx where the start idx is not found in the start spans\n",
    "                                of the text\n",
    "        list end_value_error: example idx where the end idx is not found in the end spans\n",
    "                              of the text\n",
    "        list assert_error: examples that fail assertion errors. A majority are due to the above errors\n",
    "        \n",
    "    '''\n",
    "\n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser','tagger','ner'])]\n",
    "\n",
    "        start_token = answer_tokens[0]\n",
    "        end_token = answer_tokens[-1]\n",
    "        \n",
    "        context_span  = [(word.idx, word.idx + len(word.text)) \n",
    "                         for word in nlp(row['context'], disable=['parser','tagger','ner'])]\n",
    "\n",
    "        starts, ends = zip(*context_span)\n",
    "\n",
    "        answer_start, answer_end = row['label']\n",
    "\n",
    "        try:\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "        try:\n",
    "            end_idx  = ends.index(answer_end)\n",
    "        except:\n",
    "            end_value_error.append(index)\n",
    "\n",
    "        try:\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "\n",
    "\n",
    "    return start_value_error, end_value_error, assert_error\n",
    "\n",
    "\n",
    "\n",
    "def get_error_indices(df, idx2word):\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Number of error indices: {len(err_idx)}\")\n",
    "    \n",
    "    return err_idx\n",
    "\n",
    "\n",
    "\n",
    "def index_answer(row, idx2word):\n",
    "    '''\n",
    "    Takes in a row of the dataframe or one training example and\n",
    "    returns a tuple of start and end positions of answer by calculating \n",
    "    spans.\n",
    "    '''\n",
    "    \n",
    "    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','tagger','ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    " \n",
    "    end_idx  = ends.index(answer_end)\n",
    "    \n",
    "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','tagger','ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end = ans_toks[-1]\n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]] == ans_end\n",
    "    \n",
    "    return [start_idx, end_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ff5b6",
   "metadata": {
    "papermill": {
     "duration": 0.015427,
     "end_time": "2024-04-27T16:28:33.753914",
     "exception": false,
     "start_time": "2024-04-27T16:28:33.738487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DrQA \n",
    "\n",
    "Implementation of model proposed in the paper: [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051) which is called DrQA by the authors. Specifically, DrQA is an end-to-end system for open domain question answering which involves an information retrieval system as well.  However we only implemented the deep learning model proposed by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4a814d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:33.786675Z",
     "iopub.status.busy": "2024-04-27T16:28:33.786112Z",
     "iopub.status.idle": "2024-04-27T16:28:36.474191Z",
     "shell.execute_reply": "2024-04-27T16:28:36.473101Z"
    },
    "papermill": {
     "duration": 2.707876,
     "end_time": "2024-04-27T16:28:36.476867",
     "exception": false,
     "start_time": "2024-04-27T16:28:33.768991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152ec35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:36.509099Z",
     "iopub.status.busy": "2024-04-27T16:28:36.508426Z",
     "iopub.status.idle": "2024-04-27T16:28:38.631210Z",
     "shell.execute_reply": "2024-04-27T16:28:38.629758Z"
    },
    "papermill": {
     "duration": 2.141437,
     "end_time": "2024-04-27T16:28:38.633604",
     "exception": false,
     "start_time": "2024-04-27T16:28:36.492167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  University_of_Notre_Dame\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Super_Bowl_50\n",
      "Train list len:  87599\n",
      "Valid list len:  34726\n"
     ]
    }
   ],
   "source": [
    "# load dataset json files\n",
    "\n",
    "train_data = load_json('/kaggle/input/drqna-squad/squad_train.json')\n",
    "valid_data = load_json('/kaggle/input/drqna-squad/squad_dev.json')\n",
    "\n",
    "# parse the json structure to return the data as a list of dictionaries\n",
    "\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)\n",
    "\n",
    "print('Train list len: ',len(train_list))\n",
    "print('Valid list len: ',len(valid_list))\n",
    "\n",
    "# converting the lists into dataframes\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6366d73a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:38.669639Z",
     "iopub.status.busy": "2024-04-27T16:28:38.668865Z",
     "iopub.status.idle": "2024-04-27T16:28:44.804783Z",
     "shell.execute_reply": "2024-04-27T16:28:44.803641Z"
    },
    "papermill": {
     "duration": 6.156873,
     "end_time": "2024-04-27T16:28:44.807454",
     "exception": false,
     "start_time": "2024-04-27T16:28:38.650581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_spaces(text):\n",
    "    '''\n",
    "    Removes extra white spaces from the context.\n",
    "    '''\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "train_df.context = train_df.context.apply(normalize_spaces)\n",
    "valid_df.context = valid_df.context.apply(normalize_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee6e12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:44.842287Z",
     "iopub.status.busy": "2024-04-27T16:28:44.841911Z",
     "iopub.status.idle": "2024-04-27T16:28:44.926549Z",
     "shell.execute_reply": "2024-04-27T16:28:44.925536Z"
    },
    "papermill": {
     "duration": 0.104937,
     "end_time": "2024-04-27T16:28:44.929036",
     "exception": false,
     "start_time": "2024-04-27T16:28:44.824099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4f2a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:44.965980Z",
     "iopub.status.busy": "2024-04-27T16:28:44.965098Z",
     "iopub.status.idle": "2024-04-27T16:28:45.504799Z",
     "shell.execute_reply": "2024-04-27T16:28:45.503736Z"
    },
    "papermill": {
     "duration": 0.560338,
     "end_time": "2024-04-27T16:28:45.507536",
     "exception": false,
     "start_time": "2024-04-27T16:28:44.947198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 447 ms, sys: 19 ms, total: 466 ms\n",
      "Wall time: 468 ms\n",
      "Number of sentences in dataset:  118852\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "\n",
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf22bbd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:28:45.544160Z",
     "iopub.status.busy": "2024-04-27T16:28:45.543212Z",
     "iopub.status.idle": "2024-04-27T16:38:10.190280Z",
     "shell.execute_reply": "2024-04-27T16:38:10.189207Z"
    },
    "papermill": {
     "duration": 564.683789,
     "end_time": "2024-04-27T16:38:10.208981",
     "exception": false,
     "start_time": "2024-04-27T16:28:45.525192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 110483\n",
      "vocab-length: 110485\n",
      "word2idx-length: 110485\n",
      "CPU times: user 9min 23s, sys: 794 ms, total: 9min 24s\n",
      "Wall time: 9min 24s\n"
     ]
    }
   ],
   "source": [
    "# build word vocabulary\n",
    "\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8907e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T16:38:10.244584Z",
     "iopub.status.busy": "2024-04-27T16:38:10.243898Z",
     "iopub.status.idle": "2024-04-27T17:08:45.137317Z",
     "shell.execute_reply": "2024-04-27T17:08:45.136097Z"
    },
    "papermill": {
     "duration": 1834.931536,
     "end_time": "2024-04-27T17:08:45.157315",
     "exception": false,
     "start_time": "2024-04-27T16:38:10.225779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 14s, sys: 322 ms, total: 17min 14s\n",
      "Wall time: 17min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 58s, sys: 146 ms, total: 6min 59s\n",
      "Wall time: 6min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 84 ms, total: 4min 33s\n",
      "Wall time: 4min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 29.1 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions for training and validation set\n",
    "\n",
    "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "\n",
    "%time train_df['question_ids'] = train_df.question.apply(question_to_ids,  word2idx=word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids,  word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655cf633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T17:08:45.250257Z",
     "iopub.status.busy": "2024-04-27T17:08:45.249829Z",
     "iopub.status.idle": "2024-04-27T17:39:05.407682Z",
     "shell.execute_reply": "2024-04-27T17:39:05.406581Z"
    },
    "papermill": {
     "duration": 1820.195916,
     "end_time": "2024-04-27T17:39:05.427232",
     "exception": false,
     "start_time": "2024-04-27T17:08:45.231316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 362\n"
     ]
    }
   ],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2e2b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T17:39:05.465539Z",
     "iopub.status.busy": "2024-04-27T17:39:05.465198Z",
     "iopub.status.idle": "2024-04-27T18:08:10.477686Z",
     "shell.execute_reply": "2024-04-27T18:08:10.476768Z"
    },
    "papermill": {
     "duration": 1745.034644,
     "end_time": "2024-04-27T18:08:10.480468",
     "exception": false,
     "start_time": "2024-04-27T17:39:05.445824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7226626",
   "metadata": {
    "papermill": {
     "duration": 0.017658,
     "end_time": "2024-04-27T18:08:10.516556",
     "exception": false,
     "start_time": "2024-04-27T18:08:10.498898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dump data to pickle files \n",
    "This ensures that we can directly access the preprocessed dataframe next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5944247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:10.554768Z",
     "iopub.status.busy": "2024-04-27T18:08:10.554375Z",
     "iopub.status.idle": "2024-04-27T18:08:11.869721Z",
     "shell.execute_reply": "2024-04-27T18:08:11.868769Z"
    },
    "papermill": {
     "duration": 1.337445,
     "end_time": "2024-04-27T18:08:11.872415",
     "exception": false,
     "start_time": "2024-04-27T18:08:10.534970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drqastoi.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "    \n",
    "train_df.to_pickle('drqatrain.pkl')\n",
    "valid_df.to_pickle('drqavalid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266463b0",
   "metadata": {
    "papermill": {
     "duration": 0.018284,
     "end_time": "2024-04-27T18:08:11.909202",
     "exception": false,
     "start_time": "2024-04-27T18:08:11.890918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read data from pickle files\n",
    "\n",
    "You only need to run the preprocessing once. Some preprocessing functions can take upto 3 mins. Therefore, pickling preprocessed data can save a lot of time.\n",
    "Once the preprocessed files are saved, you can directly start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9636a9c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:11.949125Z",
     "iopub.status.busy": "2024-04-27T18:08:11.948044Z",
     "iopub.status.idle": "2024-04-27T18:08:15.021157Z",
     "shell.execute_reply": "2024-04-27T18:08:15.020242Z"
    },
    "papermill": {
     "duration": 3.095957,
     "end_time": "2024-04-27T18:08:15.023939",
     "exception": false,
     "start_time": "2024-04-27T18:08:11.927982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('drqatrain.pkl')\n",
    "valid_df = pd.read_pickle('drqavalid.pkl')\n",
    "with open('drqastoi.pickle', 'rb') as handle:\n",
    "    word2idx = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce628e0",
   "metadata": {
    "papermill": {
     "duration": 0.018109,
     "end_time": "2024-04-27T18:08:15.060526",
     "exception": false,
     "start_time": "2024-04-27T18:08:15.042417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset/ Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffc1535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:15.098197Z",
     "iopub.status.busy": "2024-04-27T18:08:15.097396Z",
     "iopub.status.idle": "2024-04-27T18:08:15.177303Z",
     "shell.execute_reply": "2024-04-27T18:08:15.176441Z"
    },
    "papermill": {
     "duration": 0.101344,
     "end_time": "2024-04-27T18:08:15.179935",
     "exception": false,
     "start_time": "2024-04-27T18:08:15.078591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    -Divides the dataframe in batches.\n",
    "    -Pads the contexts and questions dynamically for each batch by padding \n",
    "     the examples to the maximum-length sequence in that batch.\n",
    "    -Calculates masks for context and question.\n",
    "    -Calculates spans for contexts.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :context_mask & question_mask: zero-mask for question and context\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while validation to calculate metrics\n",
    "        :context_spans: spans of context text\n",
    "        :ids: question_ids used in evaluation\n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "                            \n",
    "            spans = []\n",
    "            context_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                context_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i,: len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            context_mask = torch.eq(padded_context, 1)\n",
    "            question_mask = torch.eq(padded_question, 1)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            \n",
    "            yield (padded_context, padded_question, context_mask, \n",
    "                   question_mask, label, context_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481a46a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:15.216861Z",
     "iopub.status.busy": "2024-04-27T18:08:15.216493Z",
     "iopub.status.idle": "2024-04-27T18:08:15.389768Z",
     "shell.execute_reply": "2024-04-27T18:08:15.388852Z"
    },
    "papermill": {
     "duration": 0.194277,
     "end_time": "2024-04-27T18:08:15.392251",
     "exception": false,
     "start_time": "2024-04-27T18:08:15.197974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_df, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9297072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:15.429654Z",
     "iopub.status.busy": "2024-04-27T18:08:15.428887Z",
     "iopub.status.idle": "2024-04-27T18:08:15.543739Z",
     "shell.execute_reply": "2024-04-27T18:08:15.542838Z"
    },
    "papermill": {
     "duration": 0.136368,
     "end_time": "2024-04-27T18:08:15.546315",
     "exception": false,
     "start_time": "2024-04-27T18:08:15.409947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_dataset = SquadDataset(valid_df, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e89be342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:15.587485Z",
     "iopub.status.busy": "2024-04-27T18:08:15.586570Z",
     "iopub.status.idle": "2024-04-27T18:08:16.169648Z",
     "shell.execute_reply": "2024-04-27T18:08:16.168492Z"
    },
    "papermill": {
     "duration": 0.606407,
     "end_time": "2024-04-27T18:08:16.172443",
     "exception": false,
     "start_time": "2024-04-27T18:08:15.566036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f0921f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:16.212885Z",
     "iopub.status.busy": "2024-04-27T18:08:16.212490Z",
     "iopub.status.idle": "2024-04-27T18:08:16.286578Z",
     "shell.execute_reply": "2024-04-27T18:08:16.285471Z"
    },
    "papermill": {
     "duration": 0.097013,
     "end_time": "2024-04-27T18:08:16.289274",
     "exception": false,
     "start_time": "2024-04-27T18:08:16.192261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 253]),\n",
       " torch.Size([32, 19]),\n",
       " torch.Size([32, 253]),\n",
       " torch.Size([32, 19]),\n",
       " torch.Size([32, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape, a[1].shape, a[2].shape, a[3].shape, a[4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cabdb",
   "metadata": {
    "papermill": {
     "duration": 0.018586,
     "end_time": "2024-04-27T18:08:16.327472",
     "exception": false,
     "start_time": "2024-04-27T18:08:16.308886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011c978",
   "metadata": {
    "papermill": {
     "duration": 0.018375,
     "end_time": "2024-04-27T18:08:16.365009",
     "exception": false,
     "start_time": "2024-04-27T18:08:16.346634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "An input example during training is comprised of \n",
    "* a paragraph / context $p$ consisting of $l$ tokens { $p_{1}$, $p_{2}$,..., $p_{l}$ }\n",
    "* a question $q$ consisting of $m$ tokens { $q_{1}$, $q_{2}$,..., $q_{m}$ }\n",
    "* a start and and end position that comes from the context itself. More specifically, the start and end indices of the answer from the context  \n",
    "\n",
    "### Word Embedding\n",
    "Pretrained Gove embeddings are used to train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cae32f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:16.404286Z",
     "iopub.status.busy": "2024-04-27T18:08:16.403619Z",
     "iopub.status.idle": "2024-04-27T18:08:16.475134Z",
     "shell.execute_reply": "2024-04-27T18:08:16.474097Z"
    },
    "papermill": {
     "duration": 0.094197,
     "end_time": "2024-04-27T18:08:16.477827",
     "exception": false,
     "start_time": "2024-04-27T18:08:16.383630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_glove_matrix():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = vector\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f0f88b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:08:16.518527Z",
     "iopub.status.busy": "2024-04-27T18:08:16.517587Z",
     "iopub.status.idle": "2024-04-27T18:11:47.074506Z",
     "shell.execute_reply": "2024-04-27T18:11:47.073489Z"
    },
    "papermill": {
     "duration": 210.581144,
     "end_time": "2024-04-27T18:11:47.078354",
     "exception": false,
     "start_time": "2024-04-27T18:08:16.497210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove_dict = create_glove_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04ab928d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:47.128782Z",
     "iopub.status.busy": "2024-04-27T18:11:47.127787Z",
     "iopub.status.idle": "2024-04-27T18:11:47.204313Z",
     "shell.execute_reply": "2024-04-27T18:11:47.203260Z"
    },
    "papermill": {
     "duration": 0.101285,
     "end_time": "2024-04-27T18:11:47.206920",
     "exception": false,
     "start_time": "2024-04-27T18:11:47.105635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_word_embedding(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 300))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "    return weights_matrix, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b245e2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:47.252520Z",
     "iopub.status.busy": "2024-04-27T18:11:47.251865Z",
     "iopub.status.idle": "2024-04-27T18:11:47.746475Z",
     "shell.execute_reply": "2024-04-27T18:11:47.745439Z"
    },
    "papermill": {
     "duration": 0.522248,
     "end_time": "2024-04-27T18:11:47.749062",
     "exception": false,
     "start_time": "2024-04-27T18:11:47.226814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_matrix, words_found = create_word_embedding(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbbbc4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:47.788000Z",
     "iopub.status.busy": "2024-04-27T18:11:47.787255Z",
     "iopub.status.idle": "2024-04-27T18:11:47.856808Z",
     "shell.execute_reply": "2024-04-27T18:11:47.855676Z"
    },
    "papermill": {
     "duration": 0.091311,
     "end_time": "2024-04-27T18:11:47.859242",
     "exception": false,
     "start_time": "2024-04-27T18:11:47.767931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words found in glove vocab:  91272\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words found in glove vocab: \", words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fba4dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:47.896997Z",
     "iopub.status.busy": "2024-04-27T18:11:47.896606Z",
     "iopub.status.idle": "2024-04-27T18:11:48.169181Z",
     "shell.execute_reply": "2024-04-27T18:11:48.168262Z"
    },
    "papermill": {
     "duration": 0.294257,
     "end_time": "2024-04-27T18:11:48.171787",
     "exception": false,
     "start_time": "2024-04-27T18:11:47.877530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('drqaglove_vt.npy',weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdfbf5",
   "metadata": {
    "papermill": {
     "duration": 0.017849,
     "end_time": "2024-04-27T18:11:48.208203",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.190354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4e757",
   "metadata": {
    "papermill": {
     "duration": 0.017908,
     "end_time": "2024-04-27T18:11:48.243909",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.226001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Align Question Embedding\n",
    "\n",
    "The paper has different encoding procedures for the context and the question. The context/paragraph comprises of following additional features:\n",
    "\n",
    "* exact match : encodes a binary feature if $p$ can be exactly matched to one word in question in its original, lemma or lowercase form\n",
    "* token features : Includes POS, NER and TF of context tokens and\n",
    "* aligned question embedding ($f_{align}$) .  \n",
    "\n",
    "In this implementation we have only implemented the aligned question embedding.\n",
    "$f_{align}$ has been formulated as shown below:\n",
    "\n",
    "$$ f_{align} = \\sum_{j}a_{i,j}E(q_{j}) $$ \n",
    "\n",
    "where $E()$ represents the glove embeddings and\n",
    "\n",
    "where $\\alpha()$ is a single dense layer with relu non-linearity. \n",
    "\n",
    "#### Intuition\n",
    "This feature enables the model to understand what portion of the context is more important or relevant with respect to the question. The products of projections taken at token level ensure a higher value when similar words from the question and context are multiplied. Quoting the paper,\n",
    "> *these features add soft alignments between similar but non-identical words (e.g., car and vehicle).* \n",
    "\n",
    "This is achieved via backpropation and training the weights of the dense layer. While this might seem a bit weird initially, we have to trust the process of backpropogation.   \n",
    "\n",
    "While implementing, we first calculate the projections of context and question vectors. We then use `torch.bmm` to calculate the product in the numerator of $a_{i,j}$, mask the product and then pass it through the softmax function to get $a_{i,j}$. Finally, we multiply this with the question embeddings. The output of this layer is an additional context embedding which is then concatenated with the glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97edd45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:48.282516Z",
     "iopub.status.busy": "2024-04-27T18:11:48.282127Z",
     "iopub.status.idle": "2024-04-27T18:11:48.358116Z",
     "shell.execute_reply": "2024-04-27T18:11:48.357267Z"
    },
    "papermill": {
     "duration": 0.097993,
     "end_time": "2024-04-27T18:11:48.360538",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.262545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlignQuestionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, context, question, question_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, emb_dim]\n",
    "        # question = [bs, qtn_len, emb_dim]\n",
    "        # question_mask = [bs, qtn_len]\n",
    "    \n",
    "        ctx_ = self.linear(context)\n",
    "        ctx_ = self.relu(ctx_)\n",
    "        # ctx_ = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        qtn_ = self.linear(question)\n",
    "        qtn_ = self.relu(qtn_)\n",
    "        # qtn_ = [bs, qtn_len, emb_dim]\n",
    "        \n",
    "        qtn_transpose = qtn_.permute(0,2,1)\n",
    "        # qtn_transpose = [bs, emb_dim, qtn_len]\n",
    "        \n",
    "        align_scores = torch.bmm(ctx_, qtn_transpose)\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        qtn_mask = question_mask.unsqueeze(1).expand(align_scores.size())\n",
    "        # qtn_mask = [bs, 1, qtn_len] => [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        # Fills elements of self tensor(align_scores) with value(-float(inf)) where mask is True. \n",
    "        # The shape of mask must be broadcastable with the shape of the underlying tensor.\n",
    "        align_scores = align_scores.masked_fill(qtn_mask == 1, -float('inf'))\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_scores_flat = align_scores.view(-1, question.size(1))\n",
    "        # align_scores = [bs*ctx_len, qtn_len]\n",
    "        \n",
    "        alpha = F.softmax(align_scores_flat, dim=1)\n",
    "        alpha = alpha.view(-1, context.shape[1], question.shape[1])\n",
    "        # alpha = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_embedding = torch.bmm(alpha, question)\n",
    "        # align = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        return align_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa28733",
   "metadata": {
    "papermill": {
     "duration": 0.017186,
     "end_time": "2024-04-27T18:11:48.395867",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.378681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stacked BiLSTM\n",
    "\n",
    "The paragraph/context encoding which now has two features (glove and $f_{align}$) is then passed to a multilayer (3 layers) bidirectional LSTM. According to the paper,\n",
    "\n",
    "> *Specically, we choose to use a multi-layer bidirectional long short-term memory network (LSTM), and take the concatenation of each layers hidden units in the end. *\n",
    "\n",
    "To achieve this functionality we cannot directly use the pytorch recurrent layers. Every recurrent layer in pytorch returns a tuple `[output, hidden]` where `output` holds the hidden states of all the timesteps from the __last layer only__. We need to access the hidden states of intermediate layers and then concatenate them at the end.\n",
    "The following figure illustrates this point in more detail.\n",
    "\n",
    "<!-- <img src=\"images/bilstm.png\" width=\"700\" height=\"600\"/> -->\n",
    "\n",
    "This figure shows a 3-layer bidirectional LSTM with an input sequence of size $n$. The green blocks denote the forward LSTMs and the blue blocks backward. Each block is labelled with the value that it calculates. The subscript denotes the time-step and the superscript denotes the depth or the layer-number. For example $hf_{1}^{(0)}$ calculates the first hidden state in forward LSTM in the first layer. \n",
    "\n",
    "As highlighted in the diagram, we need the intermediate hidden states passed between the layers along with the final output. To create this in code, we create a `nn.ModuleList` and add 3 LSTM layers to it. The input size of the first layer remains the same but for subsequent LSTMs the input size must be twice the hidden size. This is because the `output` of the first LSTM will have the dimension of `[batch_size, seq_len, hidden_size*num_directions]` and `num_directions` is 2 in our case. In the forward method, we loop through the LSTMs, store the hidden states of each layer and finally return the concatenated output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe26a79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:48.432464Z",
     "iopub.status.busy": "2024-04-27T18:11:48.431691Z",
     "iopub.status.idle": "2024-04-27T18:11:48.505128Z",
     "shell.execute_reply": "2024-04-27T18:11:48.503989Z"
    },
    "papermill": {
     "duration": 0.094445,
     "end_time": "2024-04-27T18:11:48.507970",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.413525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackedBiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            \n",
    "            self.lstms.append(nn.LSTM(input_dim, hidden_dim,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "           \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, feature_dim]\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            lstm_input = outputs[-1]\n",
    "            lstm_out = F.dropout(lstm_input, p=self.dropout)\n",
    "            lstm_out, (hidden, cell) = self.lstms[i](lstm_input)\n",
    "           \n",
    "            outputs.append(lstm_out)\n",
    "\n",
    "    \n",
    "        output = torch.cat(outputs[1:], dim=2)\n",
    "        # [bs, seq_len, num_layers*num_dir*hidden_dim]\n",
    "        \n",
    "        output = F.dropout(output, p=self.dropout)\n",
    "      \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df364f6",
   "metadata": {
    "papermill": {
     "duration": 0.018258,
     "end_time": "2024-04-27T18:11:48.544550",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.526292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Linear Attention Layer\n",
    "This layer is used to calculate the importance of each word in the question. This can be achieved by simply taking a softmax over the input. However to add more learning capacity to the model, the inputs are multiplied by a trainable weight vector $w$ and then passed through a softmax function.  \n",
    "\n",
    "Essentially the layer is performing \"attention\" on inputs. The $w$ in code is characterized by a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c628bc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:48.585580Z",
     "iopub.status.busy": "2024-04-27T18:11:48.584627Z",
     "iopub.status.idle": "2024-04-27T18:11:48.656553Z",
     "shell.execute_reply": "2024-04-27T18:11:48.655680Z"
    },
    "papermill": {
     "duration": 0.096128,
     "end_time": "2024-04-27T18:11:48.659177",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.563049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, question, question_mask):\n",
    "        \n",
    "        # question = [bs, qtn_len, input_dim] = [bs, qtn_len, bi_lstm_hid_dim]\n",
    "        # question_mask = [bs,  qtn_len]\n",
    "        \n",
    "        qtn = question.view(-1, question.shape[-1])\n",
    "        # qtn = [bs*qtn_len, hid_dim]\n",
    "        \n",
    "        attn_scores = self.linear(qtn)\n",
    "        # attn_scores = [bs*qtn_len, 1]\n",
    "        \n",
    "        attn_scores = attn_scores.view(question.shape[0], question.shape[1])\n",
    "        # attn_scores = [bs, qtn_len]\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(question_mask == 1, -float('inf'))\n",
    "        \n",
    "        alpha = F.softmax(attn_scores, dim=1)\n",
    "        # alpha = [bs, qtn_len]\n",
    "        \n",
    "        return alpha\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2ebd1",
   "metadata": {
    "papermill": {
     "duration": 0.019213,
     "end_time": "2024-04-27T18:11:48.697146",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.677933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following function just multiplies the weights calculated in the previous layer by the outputs of the question bilstm layer. This allows the model to assign higher values to important words in each question.\n",
    "\n",
    "$$ q = \\sum_{j} b_{j} q_{j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbc95a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:48.736535Z",
     "iopub.status.busy": "2024-04-27T18:11:48.736163Z",
     "iopub.status.idle": "2024-04-27T18:11:48.808641Z",
     "shell.execute_reply": "2024-04-27T18:11:48.807668Z"
    },
    "papermill": {
     "duration": 0.095205,
     "end_time": "2024-04-27T18:11:48.811215",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.716010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(x, weights):\n",
    "    # x = [bs, len, dim]\n",
    "    # weights = [bs, len]\n",
    "    \n",
    "    weights = weights.unsqueeze(1)\n",
    "    # weights = [bs, 1, len]\n",
    "    \n",
    "    w = weights.bmm(x).squeeze(1)\n",
    "    # w = [bs, 1, dim] => [bs, dim]\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0535c4d",
   "metadata": {
    "papermill": {
     "duration": 0.017613,
     "end_time": "2024-04-27T18:11:48.847622",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.830009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bilinear Attention in NLP\n",
    "\n",
    "$$ e_{t} = s^{T} W h_{t}$$\n",
    "where $W$ is a trainable weight vector.\n",
    "This is the method used in this paper to predict the start and end position of the answer from the context.    \n",
    "\n",
    "\n",
    "To implement this layer, we characterise $W$ by a linear layer.\n",
    "First the linear layer is applied to the question, which is equivalent to the product $W.q$. This product is then multiplied by the context using `torch.bmm`.   \n",
    "Note that softmax is not taken over here to get the weights. This is taken care of when we calculate the loss using cross entropy. The following layer does not actually calculate the attention as a weighted sum. It just uses the bilinear term's representation to predict the span.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d23e30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:48.885549Z",
     "iopub.status.busy": "2024-04-27T18:11:48.884611Z",
     "iopub.status.idle": "2024-04-27T18:11:48.955970Z",
     "shell.execute_reply": "2024-04-27T18:11:48.954854Z"
    },
    "papermill": {
     "duration": 0.093414,
     "end_time": "2024-04-27T18:11:48.958649",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.865235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BilinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_dim, question_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(question_dim, context_dim)\n",
    "        \n",
    "    def forward(self, context, question, context_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, ctx_hid_dim] = [bs, ctx_len, hid_dim*6] = [bs, ctx_len, 768]\n",
    "        # question = [bs, qtn_hid_dim] = [bs, qtn_len, 768]\n",
    "        # context_mask = [bs, ctx_len]\n",
    "        \n",
    "        qtn_proj = self.linear(question)\n",
    "        # qtn_proj = [bs, ctx_hid_dim]\n",
    "        \n",
    "        qtn_proj = qtn_proj.unsqueeze(2)\n",
    "        # qtn_proj = [bs, ctx_hid_dim, 1]\n",
    "        \n",
    "        scores = context.bmm(qtn_proj)\n",
    "        # scores = [bs, ctx_len, 1]\n",
    "        \n",
    "        scores = scores.squeeze(2)\n",
    "        # scores = [bs, ctx_len]\n",
    "        \n",
    "        scores = scores.masked_fill(context_mask == 1, -float('inf'))\n",
    "        \n",
    "        #alpha = F.log_softmax(scores, dim=1)\n",
    "        # alpha = [bs, ctx_len]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedfbe2",
   "metadata": {
    "papermill": {
     "duration": 0.018214,
     "end_time": "2024-04-27T18:11:48.995644",
     "exception": false,
     "start_time": "2024-04-27T18:11:48.977430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Putting it together\n",
    "\n",
    "The following module brings all the components discussed so far together. It takes in the context and question tokens as inputs and returns the start and end positions of the answer from the context.  \n",
    "\n",
    "* Aligned question embedding is calculated for the context vector and concatenated (using `torch.cat`) to the context representation. If $d$ is the embedding dimension then context $\\epsilon$ $R^{2d}$ and question $\\epsilon$ $R^{d}$.\n",
    "* Context and question representations are then passed to bilstm layers to obtain tensors of dimension `[batch_size, seq_len, hidden_dim*6]` since the LSTM is bidirectional and there are 3 layers of it.\n",
    "* The embedded question is also passed through the linear attention layer and a weighted sum of its output is taken with the biLSTM output.\n",
    "* Both these representations are finally passed through the bilinear attention layer to predict the start and end position of the answer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c20ef6f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:49.035074Z",
     "iopub.status.busy": "2024-04-27T18:11:49.034296Z",
     "iopub.status.idle": "2024-04-27T18:11:49.115739Z",
     "shell.execute_reply": "2024-04-27T18:11:49.114904Z"
    },
    "papermill": {
     "duration": 0.104071,
     "end_time": "2024-04-27T18:11:49.118251",
     "exception": false,
     "start_time": "2024-04-27T18:11:49.014180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocumentReader(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, num_directions, dropout, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #self.embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.context_bilstm = StackedBiLSTM(embedding_dim * 2, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.question_bilstm = StackedBiLSTM(embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.glove_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        def tune_embedding(grad, words=1000):\n",
    "            grad[words:] = 0\n",
    "            return grad\n",
    "        \n",
    "        self.glove_embedding.weight.register_hook(tune_embedding)\n",
    "        \n",
    "        self.align_embedding = AlignQuestionEmbedding(embedding_dim)\n",
    "        \n",
    "        self.linear_attn_question = LinearAttentionLayer(hidden_dim*num_layers*num_directions) \n",
    "        \n",
    "        self.bilinear_attn_start = BilinearAttentionLayer(hidden_dim*num_layers*num_directions, \n",
    "                                                          hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.bilinear_attn_end = BilinearAttentionLayer(hidden_dim*num_layers*num_directions,\n",
    "                                                        hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "        \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = np.load('drqaglove_vt.npy')\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=False)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def forward(self, context, question, context_mask, question_mask):\n",
    "       \n",
    "        # context = [bs, len_c]\n",
    "        # question = [bs, len_q]\n",
    "        # context_mask = [bs, len_c]\n",
    "        # question_mask = [bs, len_q]\n",
    "        \n",
    "        \n",
    "        ctx_embed = self.glove_embedding(context)\n",
    "        # ctx_embed = [bs, len_c, emb_dim]\n",
    "        \n",
    "        ques_embed = self.glove_embedding(question)\n",
    "        # ques_embed = [bs, len_q, emb_dim]\n",
    "        \n",
    "\n",
    "        ctx_embed = self.dropout(ctx_embed)\n",
    "     \n",
    "        ques_embed = self.dropout(ques_embed)\n",
    "             \n",
    "        align_embed = self.align_embedding(ctx_embed, ques_embed, question_mask)\n",
    "        # align_embed = [bs, len_c, emb_dim]  \n",
    "        \n",
    "        ctx_bilstm_input = torch.cat([ctx_embed, align_embed], dim=2)\n",
    "        # ctx_bilstm_input = [bs, len_c, emb_dim*2]\n",
    "                \n",
    "        ctx_outputs = self.context_bilstm(ctx_bilstm_input)\n",
    "        # ctx_outputs = [bs, len_c, hid_dim*layers*dir] = [bs, len_c, hid_dim*6]\n",
    "       \n",
    "        qtn_outputs = self.question_bilstm(ques_embed)\n",
    "        # qtn_outputs = [bs, len_q, hid_dim*6]\n",
    "    \n",
    "        qtn_weights = self.linear_attn_question(qtn_outputs, question_mask)\n",
    "        # qtn_weights = [bs, len_q]\n",
    "            \n",
    "        qtn_weighted = weighted_average(qtn_outputs, qtn_weights)\n",
    "        # qtn_weighted = [bs, hid_dim*6]\n",
    "        \n",
    "        start_scores = self.bilinear_attn_start(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # start_scores = [bs, len_c]\n",
    "         \n",
    "        end_scores = self.bilinear_attn_end(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # end_scores = [bs, len_c]\n",
    "        \n",
    "      \n",
    "        return start_scores, end_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418a089",
   "metadata": {
    "papermill": {
     "duration": 0.018298,
     "end_time": "2024-04-27T18:11:49.154753",
     "exception": false,
     "start_time": "2024-04-27T18:11:49.136455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###  Hyperparameters\n",
    "\n",
    "> *We use 3-layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding. Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93e45e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:49.193252Z",
     "iopub.status.busy": "2024-04-27T18:11:49.192428Z",
     "iopub.status.idle": "2024-04-27T18:11:49.797818Z",
     "shell.execute_reply": "2024-04-27T18:11:49.796883Z"
    },
    "papermill": {
     "duration": 0.627403,
     "end_time": "2024-04-27T18:11:49.800367",
     "exception": false,
     "start_time": "2024-04-27T18:11:49.172964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "HIDDEN_DIM = 128\n",
    "EMB_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_DIRECTIONS = 2\n",
    "DROPOUT = 0.3\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = DocumentReader(HIDDEN_DIM,\n",
    "                       EMB_DIM, \n",
    "                       NUM_LAYERS, \n",
    "                       NUM_DIRECTIONS, \n",
    "                       DROPOUT, \n",
    "                       device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571f838",
   "metadata": {
    "papermill": {
     "duration": 0.018594,
     "end_time": "2024-04-27T18:11:49.838113",
     "exception": false,
     "start_time": "2024-04-27T18:11:49.819519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c28b2c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:49.877103Z",
     "iopub.status.busy": "2024-04-27T18:11:49.876312Z",
     "iopub.status.idle": "2024-04-27T18:11:51.891679Z",
     "shell.execute_reply": "2024-04-27T18:11:51.890885Z"
    },
    "papermill": {
     "duration": 2.037608,
     "end_time": "2024-04-27T18:11:51.894289",
     "exception": false,
     "start_time": "2024-04-27T18:11:49.856681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf5c4465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:51.932389Z",
     "iopub.status.busy": "2024-04-27T18:11:51.931758Z",
     "iopub.status.idle": "2024-04-27T18:11:52.009145Z",
     "shell.execute_reply": "2024-04-27T18:11:52.007989Z"
    },
    "papermill": {
     "duration": 0.098988,
     "end_time": "2024-04-27T18:11:52.011495",
     "exception": false,
     "start_time": "2024-04-27T18:11:51.912507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,186,649 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    '''Returns the number of trainable parameters in the model.'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f96afde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:52.050207Z",
     "iopub.status.busy": "2024-04-27T18:11:52.049827Z",
     "iopub.status.idle": "2024-04-27T18:11:52.132823Z",
     "shell.execute_reply": "2024-04-27T18:11:52.131973Z"
    },
    "papermill": {
     "duration": 0.10532,
     "end_time": "2024-04-27T18:11:52.135543",
     "exception": false,
     "start_time": "2024-04-27T18:11:52.030223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting training ........\")\n",
    "    \n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    \n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through training data\n",
    "    for batch in train_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, ctx, ans, ids = batch\n",
    "        \n",
    "        # place the tensors on GPU\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "        \n",
    "        # forward pass, get the predictions\n",
    "        preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        \n",
    "        # separate labels for start and end position\n",
    "        start_label, end_label = label[:,0], label[:,1]\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_label) + F.cross_entropy(end_pred, end_label)\n",
    "        \n",
    "        # backward pass, calculates the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        \n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero the gradients to prevent them from accumulating\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45eda544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:52.176382Z",
     "iopub.status.busy": "2024-04-27T18:11:52.176005Z",
     "iopub.status.idle": "2024-04-27T18:11:52.258897Z",
     "shell.execute_reply": "2024-04-27T18:11:52.258094Z"
    },
    "papermill": {
     "duration": 0.106355,
     "end_time": "2024-04-27T18:11:52.261275",
     "exception": false,
     "start_time": "2024-04-27T18:11:52.154920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(model, valid_dataset):\n",
    "    '''\n",
    "    Performs validation.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting validation .........\")\n",
    "   \n",
    "    valid_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    # puts the model in eval mode. Turns off dropout\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in valid_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, context_text, answers, ids = batch\n",
    "\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            y1, y2 = label[:,0], label[:,1]\n",
    "\n",
    "            loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            \n",
    "            # get the start and end index positions from the model preds\n",
    "            \n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            \n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "            # stack predictions\n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "            \n",
    "            \n",
    "    em, f1 = evaluate(predictions)            \n",
    "    return valid_loss/len(valid_dataset), em, f1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb04118f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:52.300947Z",
     "iopub.status.busy": "2024-04-27T18:11:52.300536Z",
     "iopub.status.idle": "2024-04-27T18:11:52.381078Z",
     "shell.execute_reply": "2024-04-27T18:11:52.380160Z"
    },
    "papermill": {
     "duration": 0.103021,
     "end_time": "2024-04-27T18:11:52.383574",
     "exception": false,
     "start_time": "2024-04-27T18:11:52.280553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('/kaggle/input/drqna-squad/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be622c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:52.421770Z",
     "iopub.status.busy": "2024-04-27T18:11:52.421403Z",
     "iopub.status.idle": "2024-04-27T18:11:52.503570Z",
     "shell.execute_reply": "2024-04-27T18:11:52.502670Z"
    },
    "papermill": {
     "duration": 0.105147,
     "end_time": "2024-04-27T18:11:52.506396",
     "exception": false,
     "start_time": "2024-04-27T18:11:52.401249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eebb6ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T18:11:52.544998Z",
     "iopub.status.busy": "2024-04-27T18:11:52.544227Z",
     "iopub.status.idle": "2024-04-27T20:25:23.415583Z",
     "shell.execute_reply": "2024-04-27T20:25:23.414376Z"
    },
    "papermill": {
     "duration": 8010.915169,
     "end_time": "2024-04-27T20:25:23.440182",
     "exception": false,
     "start_time": "2024-04-27T18:11:52.525013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 4.89303082991192| Time: 26m 18s\n",
      "Epoch valid loss: 4.006683577816357\n",
      "Epoch EM: 46.72658467360454\n",
      "Epoch F1: 59.293232057018805\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 3.6211951912967604| Time: 25m 54s\n",
      "Epoch valid loss: 3.6315393029977487\n",
      "Epoch EM: 52.70577105014191\n",
      "Epoch F1: 64.62190699698601\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 3.184596011474943| Time: 26m 38s\n",
      "Epoch valid loss: 3.451359449707152\n",
      "Epoch EM: 55.15610217596973\n",
      "Epoch F1: 67.06409240690502\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.9029179025990093| Time: 27m 32s\n",
      "Epoch valid loss: 3.348789061303467\n",
      "Epoch EM: 56.96310312204352\n",
      "Epoch F1: 68.61195664046173\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.6828506835473336| Time: 27m 6s\n",
      "Epoch valid loss: 3.3157046962272743\n",
      "Epoch EM: 57.34153263954588\n",
      "Epoch F1: 69.3014438187849\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    torch.save(model.state_dict(), 'DrQA_model')\n",
    "\n",
    "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bca1d",
   "metadata": {
    "papermill": {
     "duration": 0.021411,
     "end_time": "2024-04-27T20:25:23.527547",
     "exception": false,
     "start_time": "2024-04-27T20:25:23.506136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "* Papers read/referenced\n",
    "    1. https://arxiv.org/abs/1704.00051\n",
    "    2. https://arxiv.org/abs/1606.02858\n",
    "    3. https://arxiv.org/abs/1409.0473\n",
    "* Other helpful links\n",
    "    1. https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "    2. https://github.com/facebookresearch/DrQA\n",
    "    3. https://github.com/hitvoice/DrQA. Special thanks to [Runqi Yang](https://github.com/hitvoice) who helped me clarify some doubts with respect to preprocessing the SQUAD dataset.\n",
    "    4. https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07. Good introduction to attention.\n",
    "    5. https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture10.pdf. The attention section of this notebook is largely inspired and derived from these slides.\n",
    "* Following links are related to debugging neural nets. Something on which I was stuck for quite some time during training these models.\n",
    "    1. https://datascience.stackexchange.com/questions/410/choosing-a-learning-rate\n",
    "    2. https://www.jeremyjordan.me/nn-learning-rate/\n",
    "    3. https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    4. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "    5. https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21\n",
    "    6. https://arxiv.org/abs/1708.07120\n",
    "    7. https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8327,
     "sourceId": 11650,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4885750,
     "sourceId": 8237025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14226.29388,
   "end_time": "2024-04-27T20:25:26.503465",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-27T16:28:20.209585",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
