{"cells":[{"cell_type":"markdown","metadata":{},"source":["## BiDAF\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:06.349167Z","iopub.status.busy":"2024-05-07T18:37:06.348552Z","iopub.status.idle":"2024-05-07T18:37:08.630313Z","shell.execute_reply":"2024-05-07T18:37:08.628532Z","shell.execute_reply.started":"2024-05-07T18:37:06.349133Z"},"trusted":true},"outputs":[],"source":["import torch \n","from torch import nn\n","import torch\n","import numpy as np\n","import pandas as pd\n","import pickle, time\n","import re, os, string, typing, gc, json\n","import torch.nn.functional as F\n","import spacy\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","# import spacy.cli\n","# spacy.cli.download(\"en_core_web_lg\")\n","# nlp = spacy.load('en')\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# from preprocess import *\n","# %load_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:08.633346Z","iopub.status.busy":"2024-05-07T18:37:08.632526Z","iopub.status.idle":"2024-05-07T18:37:08.680979Z","shell.execute_reply":"2024-05-07T18:37:08.679888Z","shell.execute_reply.started":"2024-05-07T18:37:08.633310Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re, os, string, typing, gc, json\n","import spacy\n","from collections import Counter\n","# nlp = spacy.load('en')\n","\n","\n","def load_json(path):\n","    \n","    with open(path, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        \n","    print(\"Length of data: \", len(data['data']))\n","    print(\"Data Keys: \", data['data'][0].keys())\n","    print(\"Title: \", data['data'][0]['title'])\n","    \n","    return data\n","\n","\n","\n","\n","def parse_data(data:dict)->list:\n","    \n","    data = data['data']\n","    qa_list = []\n","\n","    for paragraphs in data:\n","\n","        for para in paragraphs['paragraphs']:\n","            context = para['context']\n","\n","            for qa in para['qas']:\n","                \n","                id = qa['id']\n","                question = qa['question']\n","                \n","                for ans in qa['answers']:\n","                    answer = ans['text']\n","                    ans_start = ans['answer_start']\n","                    ans_end = ans_start + len(answer)\n","                    \n","                    qa_dict = {}\n","                    qa_dict['id'] = id\n","                    qa_dict['context'] = context\n","                    qa_dict['question'] = question\n","                    qa_dict['label'] = [ans_start, ans_end]\n","\n","                    qa_dict['answer'] = answer\n","                    qa_list.append(qa_dict)    \n","\n","    \n","    return qa_list\n","\n","\n","\n","def filter_large_examples(df):\n","    \n","    \n","    ctx_lens = []\n","    query_lens = []\n","    ans_lens = []\n","    for index, row in df.iterrows():\n","        ctx_tokens = [w.text for w in nlp(row.context, disable=['parser','ner','tagger'])]\n","        if len(ctx_tokens)>400:\n","            ctx_lens.append(row.name)\n","\n","        query_tokens = [w.text for w in nlp(row.question, disable=['parser','tagger','ner'])]\n","        if len(query_tokens)>50:\n","            query_lens.append(row.name)\n","\n","        ans_tokens = [w.text for w in nlp(row.answer, disable=['parser','tagger','ner'])]\n","        if len(ans_tokens)>30:\n","            ans_lens.append(row.name)\n","\n","        assert row.name == index\n","    \n","    return set(ans_lens + ctx_lens + query_lens)\n","\n","\n","def gather_text_for_vocab(dfs:list):\n","    \n","    \n","    text = []\n","    total = 0\n","    for df in dfs:\n","        unique_contexts = list(df.context.unique())\n","        unique_questions = list(df.question.unique())\n","        total += df.context.nunique() + df.question.nunique()\n","        text.extend(unique_contexts + unique_questions)\n","    \n","    assert len(text) == total\n","    \n","    return text\n","\n","\n","\n","\n","def build_word_vocab(vocab_text):\n","    \n","    \n","    \n","    words = []\n","    for sent in vocab_text:\n","        for word in nlp(sent, disable=['parser','tagger','ner']):\n","            words.append(word.text)\n","\n","    word_counter = Counter(words)\n","    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n","    print(f\"raw-vocab: {len(word_vocab)}\")\n","    word_vocab.insert(0, '<unk>')\n","    word_vocab.insert(1, '<pad>')\n","    print(f\"vocab-length: {len(word_vocab)}\")\n","    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n","    print(f\"word2idx-length: {len(word2idx)}\")\n","    idx2word = {v:k for k,v in word2idx.items()}\n","    \n","    \n","    return word2idx, idx2word, word_vocab\n","\n","\n","\n","\n","\n","def build_char_vocab(vocab_text):\n","    \n","    chars = []\n","    for sent in vocab_text:\n","        for ch in sent:\n","            chars.append(ch)\n","\n","    char_counter = Counter(chars)\n","    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\n","    print(f\"raw-char-vocab: {len(char_vocab)}\")\n","    high_freq_char = [char for char, count in char_counter.items() if count>=20]\n","    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\n","    print(f\"char-vocab-intersect: {len(char_vocab)}\")\n","    char_vocab.insert(0,'<unk>')\n","    char_vocab.insert(1,'<pad>')\n","    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\n","    print(f\"char2idx-length: {len(char2idx)}\")\n","    \n","    return char2idx, char_vocab\n","\n","\n","\n","def context_to_ids(text, word2idx):\n","    \n","    \n","    context_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n","    context_ids = [word2idx[word] for word in context_tokens]\n","    \n","    assert len(context_ids) == len(context_tokens)\n","    return context_ids\n","\n","\n","\n","    \n","def question_to_ids(text, word2idx):\n","    \n","    \n","    question_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n","    question_ids = [word2idx[word] for word in question_tokens]\n","    \n","    assert len(question_ids) == len(question_tokens)\n","    return question_ids\n","    \n","\n","\n","    \n","def test_indices(df, idx2word):\n","    \n","\n","    start_value_error = []\n","    end_value_error = []\n","    assert_error = []\n","    for index, row in df.iterrows():\n","\n","        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser','tagger','ner'])]\n","\n","        start_token = answer_tokens[0]\n","        end_token = answer_tokens[-1]\n","        \n","        context_span  = [(word.idx, word.idx + len(word.text)) \n","                         for word in nlp(row['context'], disable=['parser','tagger','ner'])]\n","\n","        starts, ends = zip(*context_span)\n","\n","        answer_start, answer_end = row['label']\n","\n","        try:\n","            start_idx = starts.index(answer_start)\n","        except:\n","            start_value_error.append(index)\n","        try:\n","            end_idx  = ends.index(answer_end)\n","        \n","        except:\n","            end_value_error.append(index)\n","\n","        try:\n","            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n","            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n","        except:\n","            assert_error.append(index)\n","\n","\n","    return start_value_error, end_value_error, assert_error\n","\n","\n","\n","def get_error_indices(df, idx2word):\n","    \n","    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n","    err_idx = start_value_error + end_value_error + assert_error\n","    err_idx = set(err_idx)\n","    print(f\"Number of error indices: {len(err_idx)}\")\n","    \n","    return err_idx\n","\n","\n","\n","def index_answer(row, idx2word):\n","    '''\n","    Takes in a row of the dataframe or one training example and\n","    returns a tuple of start and end positions of answer by calculating \n","    spans.\n","    '''\n","    \n","    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','tagger','ner'])]\n","    starts, ends = zip(*context_span)\n","    \n","    answer_start, answer_end = row.label\n","    start_idx = starts.index(answer_start)\n"," \n","    end_idx  = ends.index(answer_end)\n","    \n","    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','tagger','ner'])]\n","    ans_start = ans_toks[0]\n","    ans_end = ans_toks[-1]\n","    assert idx2word[row.context_ids[start_idx]] == ans_start\n","    assert idx2word[row.context_ids[end_idx]] == ans_end\n","    \n","    return [start_idx, end_idx]\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:08.682604Z","iopub.status.busy":"2024-05-07T18:37:08.682219Z","iopub.status.idle":"2024-05-07T18:37:11.541391Z","shell.execute_reply":"2024-05-07T18:37:11.540468Z","shell.execute_reply.started":"2024-05-07T18:37:08.682569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of data:  442\n","Data Keys:  dict_keys(['title', 'paragraphs'])\n","Title:  Beyoncé\n","Length of data:  35\n","Data Keys:  dict_keys(['title', 'paragraphs'])\n","Title:  Normans\n","--------------------------\n","Train list len:  86821\n","Valid list len:  20302\n"]}],"source":["# load dataset json files\n","\n","train_data = load_json('/kaggle/input/squad-train22/train-v2.0.json')\n","valid_data = load_json('/kaggle/input/squad-dev/dev-v2.0.json')\n","\n","# parse the json structure to return the data as a list of dictionaries\n","\n","train_list = parse_data(train_data)\n","valid_list = parse_data(valid_data)\n","print('--------------------------')\n","\n","print('Train list len: ',len(train_list))\n","print('Valid list len: ',len(valid_list))\n","\n","# converting the lists into dataframes\n","\n","train_df = pd.DataFrame(train_list)\n","valid_df = pd.DataFrame(valid_list)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:11.544111Z","iopub.status.busy":"2024-05-07T18:37:11.543820Z","iopub.status.idle":"2024-05-07T18:37:11.561469Z","shell.execute_reply":"2024-05-07T18:37:11.560547Z","shell.execute_reply.started":"2024-05-07T18:37:11.544084Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>context</th>\n","      <th>question</th>\n","      <th>label</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>56be85543aeaaa14008c9063</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>[269, 286]</td>\n","      <td>in the late 1990s</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>56be85543aeaaa14008c9065</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>[207, 226]</td>\n","      <td>singing and dancing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>56be85543aeaaa14008c9066</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>[526, 530]</td>\n","      <td>2003</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>56bf6b0f3aeaaa14008c9601</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>[166, 180]</td>\n","      <td>Houston, Texas</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>56bf6b0f3aeaaa14008c9602</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>[276, 286]</td>\n","      <td>late 1990s</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         id  \\\n","0  56be85543aeaaa14008c9063   \n","1  56be85543aeaaa14008c9065   \n","2  56be85543aeaaa14008c9066   \n","3  56bf6b0f3aeaaa14008c9601   \n","4  56bf6b0f3aeaaa14008c9602   \n","\n","                                             context  \\\n","0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            question       label  \\\n","0           When did Beyonce start becoming popular?  [269, 286]   \n","1  What areas did Beyonce compete in when she was...  [207, 226]   \n","2  When did Beyonce leave Destiny's Child and bec...  [526, 530]   \n","3      In what city and state did Beyonce  grow up?   [166, 180]   \n","4         In which decade did Beyonce become famous?  [276, 286]   \n","\n","                answer  \n","0    in the late 1990s  \n","1  singing and dancing  \n","2                 2003  \n","3       Houston, Texas  \n","4           late 1990s  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:11.562960Z","iopub.status.busy":"2024-05-07T18:37:11.562590Z","iopub.status.idle":"2024-05-07T18:37:11.569363Z","shell.execute_reply":"2024-05-07T18:37:11.568372Z","shell.execute_reply.started":"2024-05-07T18:37:11.562927Z"},"trusted":true},"outputs":[],"source":["def preprocess_df(df):\n","    \n","    def to_lower(text):\n","        return text.lower()\n","\n","    df.context = df.context.apply(to_lower)\n","    df.question = df.question.apply(to_lower)\n","    df.answer = df.answer.apply(to_lower)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:11.570882Z","iopub.status.busy":"2024-05-07T18:37:11.570584Z","iopub.status.idle":"2024-05-07T18:37:12.072305Z","shell.execute_reply":"2024-05-07T18:37:12.071240Z","shell.execute_reply.started":"2024-05-07T18:37:11.570856Z"},"trusted":true},"outputs":[],"source":["preprocess_df(train_df)\n","preprocess_df(valid_df)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:12.073857Z","iopub.status.busy":"2024-05-07T18:37:12.073564Z","iopub.status.idle":"2024-05-07T18:37:12.436957Z","shell.execute_reply":"2024-05-07T18:37:12.435976Z","shell.execute_reply.started":"2024-05-07T18:37:12.073831Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 349 ms, sys: 9.47 ms, total: 358 ms\n","Wall time: 357 ms\n","Number of sentences in dataset:  112750\n"]}],"source":["# gather text to build vocabularies\n","train_df = train_df\n","valid_df = valid_df\n","%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n","print(\"Number of sentences in dataset: \", len(vocab_text))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:37:12.439129Z","iopub.status.busy":"2024-05-07T18:37:12.438479Z","iopub.status.idle":"2024-05-07T18:44:59.838276Z","shell.execute_reply":"2024-05-07T18:44:59.837243Z","shell.execute_reply.started":"2024-05-07T18:37:12.439090Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"name":"stdout","output_type":"stream","text":["raw-vocab: 94385\n","vocab-length: 94387\n","word2idx-length: 94387\n","CPU times: user 7min 43s, sys: 605 ms, total: 7min 44s\n","Wall time: 7min 44s\n","----------------------------------\n","raw-char-vocab: 1308\n","char-vocab-intersect: 202\n","char2idx-length: 204\n","CPU times: user 2.8 s, sys: 114 ms, total: 2.92 s\n","Wall time: 2.91 s\n"]}],"source":["# build word and character-level vocabularies\n","\n","%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n","print(\"----------------------------------\")\n","%time char2idx, char_vocab = build_char_vocab(vocab_text)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T18:44:59.840595Z","iopub.status.busy":"2024-05-07T18:44:59.839707Z","iopub.status.idle":"2024-05-07T19:08:00.970293Z","shell.execute_reply":"2024-05-07T19:08:00.969503Z","shell.execute_reply.started":"2024-05-07T18:44:59.840555Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                         id  \\\n","0  56be85543aeaaa14008c9063   \n","\n","                                             context  \\\n","0  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...   \n","\n","                                   question       label             answer  \n","0  when did beyonce start becoming popular?  [269, 286]  in the late 1990s  \n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"name":"stdout","output_type":"stream","text":["                         id  \\\n","0  56be85543aeaaa14008c9063   \n","\n","                                             context  \\\n","0  beyoncé giselle knowles-carter (/biːˈjɒnseɪ/ b...   \n","\n","                                   question       label             answer  \\\n","0  when did beyonce start becoming popular?  [269, 286]  in the late 1990s   \n","\n","                                         context_ids  \n","0  [934, 39335, 17764, 15, 9209, 21, 52928, 11806...  \n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]}],"source":["# numericalize context and questions for training and validation set\n","print(train_df.head(1))\n","train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n","print(train_df.head(1))\n","valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n","train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)\n","valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx=word2idx)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:08:00.975707Z","iopub.status.busy":"2024-05-07T19:08:00.975354Z","iopub.status.idle":"2024-05-07T19:31:12.238190Z","shell.execute_reply":"2024-05-07T19:31:12.237138Z","shell.execute_reply.started":"2024-05-07T19:08:00.975682Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"name":"stdout","output_type":"stream","text":["Number of error indices: 905\n","Number of error indices: 198\n"]}],"source":["# get indices with tokenization errors and drop those indices \n","\n","train_err = get_error_indices(train_df, idx2word)\n","valid_err = get_error_indices(valid_df, idx2word)\n","\n","train_df.drop(train_err, inplace=True)\n","valid_df.drop(valid_err, inplace=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:31:12.240184Z","iopub.status.busy":"2024-05-07T19:31:12.239816Z","iopub.status.idle":"2024-05-07T19:53:40.486894Z","shell.execute_reply":"2024-05-07T19:53:40.485951Z","shell.execute_reply.started":"2024-05-07T19:31:12.240150Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]}],"source":["# get start and end positions of answers from the context\n","# this is basically the label for training QA models\n","\n","train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n","valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n","\n","train_df['label_idx'] = train_label_idx\n","valid_df['label_idx'] = valid_label_idx"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:40.488487Z","iopub.status.busy":"2024-05-07T19:53:40.488136Z","iopub.status.idle":"2024-05-07T19:53:40.493337Z","shell.execute_reply":"2024-05-07T19:53:40.492097Z","shell.execute_reply.started":"2024-05-07T19:53:40.488457Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:40.495227Z","iopub.status.busy":"2024-05-07T19:53:40.494799Z","iopub.status.idle":"2024-05-07T19:53:40.502611Z","shell.execute_reply":"2024-05-07T19:53:40.501544Z","shell.execute_reply.started":"2024-05-07T19:53:40.495190Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Dataloader/Dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:40.504598Z","iopub.status.busy":"2024-05-07T19:53:40.504120Z","iopub.status.idle":"2024-05-07T19:53:40.527362Z","shell.execute_reply":"2024-05-07T19:53:40.526324Z","shell.execute_reply.started":"2024-05-07T19:53:40.504563Z"},"trusted":true},"outputs":[],"source":["class SquadDataset:\n","    '''\n","    - Creates batches dynamically by padding to the length of largest example\n","      in a given batch.\n","    - Calulates character vectors for contexts and question.\n","    - Returns tensors for training.\n","    '''\n","    \n","    def __init__(self, data, batch_size):\n","        \n","        self.batch_size = batch_size\n","        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n","        self.data = data\n","        \n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n","        \n","        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n","        \n","        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n","            for j, ch in enumerate(word.text):\n","                char_vec[i][j] = char2idx.get(ch, 0)\n","        \n","        return char_vec    \n","    \n","    def get_span(self, text):\n","        \n","        text = nlp(text, disable=['parser','tagger','ner'])\n","        span = [(w.idx, w.idx+len(w.text)) for w in text]\n","\n","        return span\n","\n","    def __iter__(self):\n","        '''\n","        Creates batches of data and yields them.\n","        \n","        Each yield comprises of:\n","        :padded_context: padded tensor of contexts for each batch \n","        :padded_question: padded tensor of questions for each batch \n","        :char_ctx & ques_ctx: character-level ids for context and question\n","        :label: start and end index wrt context_ids\n","        :context_text,answer_text: used while validation to calculate metrics\n","        :ids: question_ids for evaluation\n","        \n","        '''\n","        \n","        for batch in self.data:\n","            \n","            spans = []\n","            ctx_text = []\n","            answer_text = []\n","            \n","            for ctx in batch.context:\n","                ctx_text.append(ctx)\n","                spans.append(self.get_span(ctx))\n","            \n","            for ans in batch.answer:\n","                answer_text.append(ans)\n","                \n","            \n","            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n","            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n","            \n","            for i, ctx in enumerate(batch.context_ids):\n","                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n","                \n","            max_word_ctx = 0\n","            for context in batch.context:\n","                for word in nlp(context, disable=['parser','tagger','ner']):\n","                    if len(word.text) > max_word_ctx:\n","                        max_word_ctx = len(word.text)\n","            \n","            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n","            for i, context in enumerate(batch.context):\n","                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n","            \n","            max_question_len = max([len(ques) for ques in batch.question_ids])\n","            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n","            \n","            for i, ques in enumerate(batch.question_ids):\n","                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n","                \n","            max_word_ques = 0\n","            for question in batch.question:\n","                for word in nlp(question, disable=['parser','tagger','ner']):\n","                    if len(word.text) > max_word_ques:\n","                        max_word_ques = len(word.text)\n","            \n","            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n","            for i, question in enumerate(batch.question):\n","                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n","            \n","            ids = list(batch.id)  \n","            label = torch.LongTensor(list(batch.label_idx))\n","            \n","            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n","            \n","            "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:40.529106Z","iopub.status.busy":"2024-05-07T19:53:40.528731Z","iopub.status.idle":"2024-05-07T19:53:41.309066Z","shell.execute_reply":"2024-05-07T19:53:41.308232Z","shell.execute_reply.started":"2024-05-07T19:53:40.529071Z"},"trusted":true},"outputs":[],"source":["train_dataset = SquadDataset(train_df, 16)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:41.310551Z","iopub.status.busy":"2024-05-07T19:53:41.310220Z","iopub.status.idle":"2024-05-07T19:53:41.393167Z","shell.execute_reply":"2024-05-07T19:53:41.392177Z","shell.execute_reply.started":"2024-05-07T19:53:41.310525Z"},"trusted":true},"outputs":[],"source":["valid_dataset = SquadDataset(valid_df, 16)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:41.394649Z","iopub.status.busy":"2024-05-07T19:53:41.394330Z","iopub.status.idle":"2024-05-07T19:53:42.117909Z","shell.execute_reply":"2024-05-07T19:53:42.116943Z","shell.execute_reply.started":"2024-05-07T19:53:41.394617Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## BiDAF Model"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:42.119673Z","iopub.status.busy":"2024-05-07T19:53:42.119197Z","iopub.status.idle":"2024-05-07T19:53:42.126097Z","shell.execute_reply":"2024-05-07T19:53:42.125083Z","shell.execute_reply.started":"2024-05-07T19:53:42.119638Z"},"trusted":true},"outputs":[],"source":["def get_glove_dict():\n","    '''\n","    Parses the glove word vectors text file and returns a dictionary with the words as\n","    keys and their respective pretrained word vectors as values.\n","\n","    '''\n","    glove_dict = {}\n","    with open(\"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], \"float32\")\n","            glove_dict[word] = vector\n","            \n","    f.close()\n","    \n","    return glove_dict"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:53:42.127690Z","iopub.status.busy":"2024-05-07T19:53:42.127315Z","iopub.status.idle":"2024-05-07T19:54:09.200131Z","shell.execute_reply":"2024-05-07T19:54:09.199197Z","shell.execute_reply.started":"2024-05-07T19:53:42.127659Z"},"trusted":true},"outputs":[],"source":["from gensim.downloader import load\n","\n","glove_dict = get_glove_dict()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.201767Z","iopub.status.busy":"2024-05-07T19:54:09.201189Z","iopub.status.idle":"2024-05-07T19:54:09.206188Z","shell.execute_reply":"2024-05-07T19:54:09.205197Z","shell.execute_reply.started":"2024-05-07T19:54:09.201740Z"},"trusted":true},"outputs":[],"source":["# print(glove_dict)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.207692Z","iopub.status.busy":"2024-05-07T19:54:09.207345Z","iopub.status.idle":"2024-05-07T19:54:09.214987Z","shell.execute_reply":"2024-05-07T19:54:09.214307Z","shell.execute_reply.started":"2024-05-07T19:54:09.207667Z"},"trusted":true},"outputs":[],"source":["def create_weights_matrix(glove_dict):\n","    '''\n","    Creates a weight matrix of the words that are common in the GloVe vocab and\n","    the dataset's vocab. Initializes OOV words with a zero vector.\n","    '''\n","    weights_matrix = np.zeros((len(word_vocab), 100))\n","    words_found = 0\n","    for i, word in enumerate(word_vocab):\n","        try:\n","            weights_matrix[i] = glove_dict[word]\n","            words_found += 1\n","        except:\n","            pass\n","        \n","    return weights_matrix, words_found\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.216923Z","iopub.status.busy":"2024-05-07T19:54:09.216257Z","iopub.status.idle":"2024-05-07T19:54:09.416078Z","shell.execute_reply":"2024-05-07T19:54:09.415109Z","shell.execute_reply.started":"2024-05-07T19:54:09.216888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Words found in the GloVe vocab:  71167\n"]}],"source":["weights_matrix, words_found = create_weights_matrix(glove_dict)\n","print(\"Words found in the GloVe vocab: \" ,words_found)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.418007Z","iopub.status.busy":"2024-05-07T19:54:09.417643Z","iopub.status.idle":"2024-05-07T19:54:09.477451Z","shell.execute_reply":"2024-05-07T19:54:09.476647Z","shell.execute_reply.started":"2024-05-07T19:54:09.417966Z"},"trusted":true},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.479115Z","iopub.status.busy":"2024-05-07T19:54:09.478687Z","iopub.status.idle":"2024-05-07T19:54:09.490186Z","shell.execute_reply":"2024-05-07T19:54:09.489187Z","shell.execute_reply.started":"2024-05-07T19:54:09.479083Z"},"trusted":true},"outputs":[],"source":["class CharacterEmbeddingLayer(nn.Module):\n","    \n","    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n","        \n","        super().__init__()\n","        \n","        self.char_emb_dim = char_emb_dim\n","        \n","        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n","        \n","        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n","        \n","        self.relu = nn.ReLU()\n","    \n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, x):\n","        \n","        batch_size = x.shape[0]\n","        \n","        x = self.dropout(self.char_embedding(x))\n","        # x = [bs, seq_len, word_len, char_emb_dim]\n","        \n","        # following three operations manipulate x in such a way that\n","        # it closely resembles an image. this format is important before \n","        # we perform convolution on the character embeddings.\n","        \n","        x = x.permute(0,1,3,2)\n","        # x = [bs, seq_len, char_emb_dim, word_len]\n","        \n","        x = x.view(-1, self.char_emb_dim, x.shape[3])\n","        # x = [bs*seq_len, char_emb_dim, word_len]\n","        \n","        x = x.unsqueeze(1)\n","        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n","        \n","        # x is now in a format that can be accepted by a conv layer. \n","        \n","        \n","        x = self.relu(self.char_convolution(x))\n","        # x = [bs*seq_len, out_channels, H_out, W_out]\n","        \n","        x = x.squeeze()\n","        # x = [bs*seq_len, out_channels, W_out]\n","                \n","        x = F.max_pool1d(x, x.shape[2]).squeeze()\n","        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n","        \n","        x = x.view(batch_size, -1, x.shape[-1])\n","        # x = [bs, seq_len, out_channels]\n","        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n","        \n","        \n","        return x        "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.492124Z","iopub.status.busy":"2024-05-07T19:54:09.491662Z","iopub.status.idle":"2024-05-07T19:54:09.501738Z","shell.execute_reply":"2024-05-07T19:54:09.500909Z","shell.execute_reply.started":"2024-05-07T19:54:09.492090Z"},"trusted":true},"outputs":[],"source":["class HighwayNetwork(nn.Module):\n","    \n","    def __init__(self, input_dim, num_layers=2):\n","        \n","        super().__init__()\n","        \n","        self.num_layers = num_layers\n","        \n","        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n","        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n","        \n","    def forward(self, x):\n","        \n","        for i in range(self.num_layers):\n","            \n","            flow_value = F.relu(self.flow_layer[i](x))\n","            gate_value = torch.sigmoid(self.gate_layer[i](x))\n","            \n","            x = gate_value * flow_value + (1-gate_value) * x\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.503460Z","iopub.status.busy":"2024-05-07T19:54:09.503002Z","iopub.status.idle":"2024-05-07T19:54:09.513879Z","shell.execute_reply":"2024-05-07T19:54:09.513031Z","shell.execute_reply.started":"2024-05-07T19:54:09.503427Z"},"trusted":true},"outputs":[],"source":["class ContextualEmbeddingLayer(nn.Module):\n","    \n","    def __init__(self, input_dim, hidden_dim):\n","        \n","        super().__init__()\n","        \n","        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        \n","        self.highway_net = HighwayNetwork(input_dim)\n","        \n","    def forward(self, x):\n","        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n","        # the input is the concatenation of word and characeter embeddings\n","        # for the sequence.\n","        \n","        highway_out = self.highway_net(x)\n","        # highway_out = [bs, seq_len, input_dim]\n","        \n","        outputs, _ = self.lstm(highway_out)\n","        # outputs = [bs, seq_len, emb_dim*2]\n","        \n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.515621Z","iopub.status.busy":"2024-05-07T19:54:09.515273Z","iopub.status.idle":"2024-05-07T19:54:09.536355Z","shell.execute_reply":"2024-05-07T19:54:09.535443Z","shell.execute_reply.started":"2024-05-07T19:54:09.515590Z"},"trusted":true},"outputs":[],"source":["class BiDAF(nn.Module):\n","    \n","    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n","                 kernel_size, ctx_hidden_dim, device):\n","        '''\n","        char_vocab_dim = len(char2idx)\n","        emb_dim = 100\n","        char_emb_dim = 8\n","        num_output_chanels = 100\n","        kernel_size = (8,5)\n","        ctx_hidden_dim = 100\n","        '''\n","        super().__init__()\n","        \n","        self.device = device\n","        \n","        self.word_embedding = self.get_glove_embedding()\n","        \n","        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n","                                                      num_output_channels, kernel_size)\n","        \n","        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n","        \n","        self.dropout = nn.Dropout()\n","        \n","        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n","        \n","        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n","        \n","        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n","        \n","        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n","        \n","        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n","        \n","    \n","    def get_glove_embedding(self):\n","        \n","        \n","        num_embeddings, embedding_dim = weights_matrix.shape\n","        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n","\n","        return embedding\n","        \n","    def forward(self, ctx, ques, char_ctx, char_ques):\n","        # ctx = [bs, ctx_len]\n","        # ques = [bs, ques_len]\n","        # char_ctx = [bs, ctx_len, ctx_word_len]\n","        # char_ques = [bs, ques_len, ques_word_len]\n","        \n","        ctx_len = ctx.shape[1]\n","        \n","        ques_len = ques.shape[1]\n","        \n","        ## GET WORD AND CHARACTER EMBEDDINGS\n","        \n","        ctx_word_embed = self.word_embedding(ctx)\n","        # ctx_word_embed = [bs, ctx_len, emb_dim]\n","        \n","        ques_word_embed = self.word_embedding(ques)\n","        # ques_word_embed = [bs, ques_len, emb_dim]\n","        \n","        ctx_char_embed = self.character_embedding(char_ctx)\n","        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n","        \n","        ques_char_embed = self.character_embedding(char_ques)\n","        # ques_char_embed = [bs, ques_len, emb_dim]\n","        \n","        ## CREATE CONTEXTUAL EMBEDDING\n","        \n","        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n","        # [bs, ctx_len, emb_dim*2]\n","        \n","        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n","        # [bs, ques_len, emb_dim*2]\n","        \n","        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n","        # [bs, ctx_len, emb_dim*2]\n","        \n","        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n","        # [bs, ques_len, emb_dim*2]\n","        \n","        \n","        ## CREATE SIMILARITY MATRIX\n","        \n","        ctx_ = ctx_contextual_emb.unsqueeze()\n","        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n","        \n","        ques_ = ques_contextual_emb.unsqueeze()\n","        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n","        \n","        elementwise_prod = torch.mul(ctx_, ques_)\n","        # [bs, ctx_len, ques_len, emb_dim*2]\n","        \n","        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n","        # [bs, ctx_len, ques_len, emb_dim*6]\n","        \n","        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n","        # [bs, ctx_len, ques_len]\n","        \n","        \n","        ## CALCULATE CONTEXT2QUERY ATTENTION\n","        \n","        a = F.softmax(similarity_matrix, dim=-1)\n","        # [bs, ctx_len, ques_len]\n","        \n","        c2q = torch.bmm(a, ques_contextual_emb)\n","        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n","        \n","        \n","        ## CALCULATE QUERY2CONTEXT ATTENTION\n","        \n","        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n","        # [bs, ctx_len]\n","        \n","        b = b.unsqueeze(1)\n","        # [bs, 1, ctx_len]\n","        \n","        q2c = torch.bmm(b, ctx_contextual_emb)\n","        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n","        \n","        q2c = q2c.repeat(1, ctx_len, 1)\n","        # [bs, ctx_len, emb_dim*2]\n","        \n","        ## QUERY AWARE REPRESENTATION\n","        \n","        G = torch.cat([ctx_contextual_emb, c2q, \n","                       torch.mul(ctx_contextual_emb,c2q), \n","                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n","        \n","        # [bs, ctx_len, emb_dim*8]\n","        \n","        \n","        ## MODELING LAYER\n","        \n","        M, _ = self.modeling_lstm(G)\n","        # [bs, ctx_len, emb_dim*2]\n","        \n","        ## OUTPUT LAYER\n","        \n","        M2, _ = self.end_lstm(M)\n","        \n","        # START PREDICTION\n","        \n","        p1 = self.output_start(torch.cat([G,M], dim=2))\n","        # [bs, ctx_len, 1]\n","        \n","        p1 = p1.squeeze()\n","        # [bs, ctx_len]\n","        \n","        #p1 = F.softmax(p1, dim=-1)\n","        \n","        # END PREDICTION\n","        \n","        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n","        # [bs, ctx_len, 1] => [bs, ctx_len]\n","        \n","        #p2 = F.softmax(p2, dim=-1)\n","        \n","        \n","        return p1, p2\n","    "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.543000Z","iopub.status.busy":"2024-05-07T19:54:09.542235Z","iopub.status.idle":"2024-05-07T19:54:09.871791Z","shell.execute_reply":"2024-05-07T19:54:09.870971Z","shell.execute_reply.started":"2024-05-07T19:54:09.542971Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["CHAR_VOCAB_DIM = len(char2idx)\n","EMB_DIM = 100\n","CHAR_EMB_DIM = 8\n","NUM_OUTPUT_CHANNELS = 100\n","KERNEL_SIZE = (8,5)\n","HIDDEN_DIM = 100\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","model = BiDAF(CHAR_VOCAB_DIM, \n","              EMB_DIM, \n","              CHAR_EMB_DIM, \n","              NUM_OUTPUT_CHANNELS, \n","              KERNEL_SIZE, \n","              HIDDEN_DIM, \n","              device).to(device)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:09.873178Z","iopub.status.busy":"2024-05-07T19:54:09.872889Z","iopub.status.idle":"2024-05-07T19:54:11.874797Z","shell.execute_reply":"2024-05-07T19:54:11.873925Z","shell.execute_reply.started":"2024-05-07T19:54:09.873152Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","from torch.autograd import Variable\n","optimizer = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:11.876502Z","iopub.status.busy":"2024-05-07T19:54:11.875966Z","iopub.status.idle":"2024-05-07T19:54:11.885604Z","shell.execute_reply":"2024-05-07T19:54:11.884607Z","shell.execute_reply.started":"2024-05-07T19:54:11.876474Z"},"trusted":true},"outputs":[],"source":["def train(model, train_dataset):\n","    print(\"Starting training ........\")\n","   \n","\n","    train_loss = 0.\n","    batch_count = 0\n","    model.train()\n","    for batch in train_dataset:\n","        \n","        optimizer.zero_grad()\n","    \n","        if batch_count % 500 == 0:\n","            print(f\"Starting batch: {batch_count}\")\n","        batch_count += 1\n","        \n","        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n","\n","        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n","                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n","\n","\n","        preds = model(context, question, char_ctx, char_ques)\n","\n","        start_pred, end_pred = preds\n","\n","        s_idx, e_idx = label[:,0], label[:,1]\n","\n","        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n","\n","        loss.backward()\n","        \n","\n","    \n","\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    return train_loss/len(train_dataset)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:11.887701Z","iopub.status.busy":"2024-05-07T19:54:11.886842Z","iopub.status.idle":"2024-05-07T19:54:11.901536Z","shell.execute_reply":"2024-05-07T19:54:11.900530Z","shell.execute_reply.started":"2024-05-07T19:54:11.887672Z"},"trusted":true},"outputs":[],"source":["def valid(model, valid_dataset, path):\n","    \n","    print(\"Starting Evaluating .........\")\n","    valid_loss = 0.\n","    batch_count = 0\n","    f1, em = 0., 0.\n","    model.eval()\n","    predictions = {}\n","    \n","    for batch in valid_dataset:\n","\n","        \n","        batch_count += 1\n","\n","        context, question, char_ctx, char_ques, label, ids = batch\n","\n","        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n","                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n","        \n","       \n","\n","        \n","        with torch.no_grad():\n","            \n","            s_idx, e_idx = label[:,0], label[:,1]\n","\n","            preds = model(context, question, char_ctx, char_ques)\n","\n","            p1, p2 = preds\n","\n","            \n","            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n","\n","            valid_loss += loss.item()\n","\n","            batch_size, c_len = p1.size()\n","            ls = nn.LogSoftmax(dim=1)\n","            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n","            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n","            score, s_idx = score.max(dim=1)\n","            score, e_idx = score.max(dim=1)\n","            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n","            \n","           \n","            for i in range(batch_size):\n","                id = ids[i]\n","                pred = context[i][s_idx[i]:e_idx[i]+1]\n","                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n","                predictions[id] = pred\n","            \n","\n","    if(path=='None'):\n","        return predictions\n","    em, f1 = evaluate(predictions, valid_dataset, path)\n","    return valid_loss/len(valid_dataset), em, f1"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:11.903144Z","iopub.status.busy":"2024-05-07T19:54:11.902814Z","iopub.status.idle":"2024-05-07T19:54:11.912924Z","shell.execute_reply":"2024-05-07T19:54:11.912084Z","shell.execute_reply.started":"2024-05-07T19:54:11.903117Z"},"trusted":true},"outputs":[],"source":["def evaluate(predictions, dataset, path):\n","   \n","    with open(path,'r',encoding='utf-8') as f:\n","        dataset = json.load(f)\n","        \n","    dataset = dataset['data']\n","    f1 = exact_match = total = 0\n","    for article in dataset:\n","        for paragraph in article['paragraphs']:\n","            for qa in paragraph['qas']:\n","                if qa['id'] not in predictions:\n","                    continue\n","                total += 1\n","                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n","                \n","                prediction = predictions[qa['id']]\n","                \n","                exact_match += metric_max_over_ground_truths(\n","                    exact_match_score, prediction, ground_truths)\n","                \n","                f1 += metric_max_over_ground_truths(\n","                    f1_score, prediction, ground_truths)\n","                \n","    \n","    exact_match = 100.0 * exact_match / total\n","    f1 = 100.0 * f1 / total\n","    \n","    return exact_match, f1\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:11.914975Z","iopub.status.busy":"2024-05-07T19:54:11.914531Z","iopub.status.idle":"2024-05-07T19:54:11.927721Z","shell.execute_reply":"2024-05-07T19:54:11.926845Z","shell.execute_reply.started":"2024-05-07T19:54:11.914941Z"},"trusted":true},"outputs":[],"source":["def normalize_answer(s):\n","    '''\n","    Performs a series of cleaning steps on the ground truth and \n","    predicted answer.\n","    '''\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n","    '''\n","    Returns maximum value of metrics for predicition by model against\n","    multiple ground truths.\n","    \n","    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n","    :param str prediction: predicted answer span by the model\n","    :param list ground_truths: list of ground truths against which\n","                               metrics are calculated. Maximum values of \n","                               metrics are chosen.\n","                            \n","    \n","    '''\n","    scores_for_ground_truths = []\n","    for ground_truth in ground_truths:\n","        score = metric_fn(prediction, ground_truth)\n","        scores_for_ground_truths.append(score)\n","        \n","    return max(scores_for_ground_truths)\n","\n","\n","def f1_score(prediction, ground_truth):\n","    '''\n","    Returns f1 score of two strings.\n","    '''\n","    prediction_tokens = normalize_answer(prediction).split()\n","    ground_truth_tokens = normalize_answer(ground_truth).split()\n","    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction_tokens)\n","    recall = 1.0 * num_same / len(ground_truth_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def exact_match_score(prediction, ground_truth):\n","    '''\n","    Returns exact_match_score of two strings.\n","    '''\n","    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n","\n","\n","def epoch_time(start_time, end_time):\n","    '''\n","    Helper function to record epoch time.\n","    '''\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T19:54:11.929196Z","iopub.status.busy":"2024-05-07T19:54:11.928900Z","iopub.status.idle":"2024-05-07T23:55:09.943398Z","shell.execute_reply":"2024-05-07T23:55:09.942445Z","shell.execute_reply.started":"2024-05-07T19:54:11.929154Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Starting training ........\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"name":"stdout","output_type":"stream","text":["Starting batch: 0\n","Starting batch: 500\n","Starting batch: 1000\n","Starting batch: 1500\n","Starting batch: 2000\n","Starting batch: 2500\n","Starting batch: 3000\n","Starting batch: 3500\n","Starting batch: 4000\n","Starting batch: 4500\n","Starting batch: 5000\n","Starting Evaluating .........\n","Epoch train loss : 5.673890112765008| Time: 79m 57s\n","Epoch valid loss: 5.673890112765008\n","Epoch EM: 32.81382347958665\n","Epoch F1: 45.21174811386508\n","====================================================================================\n","Epoch 2\n","Starting training ........\n","Starting batch: 0\n","Starting batch: 500\n","Starting batch: 1000\n","Starting batch: 1500\n","Starting batch: 2000\n","Starting batch: 2500\n","Starting batch: 3000\n","Starting batch: 3500\n","Starting batch: 4000\n","Starting batch: 4500\n","Starting batch: 5000\n","Starting Evaluating .........\n","Epoch train loss : 3.429111601342924| Time: 80m 0s\n","Epoch valid loss: 3.429111601342924\n","Epoch EM: 53.85397255632729\n","Epoch F1: 66.34066715961441\n","====================================================================================\n","Epoch 3\n","Starting training ........\n","Starting batch: 0\n","Starting batch: 500\n","Starting batch: 1000\n","Starting batch: 1500\n","Starting batch: 2000\n","Starting batch: 2500\n","Starting batch: 3000\n","Starting batch: 3500\n","Starting batch: 4000\n","Starting batch: 4500\n","Starting batch: 5000\n","Starting Evaluating .........\n","Epoch train loss : 2.818967187182641| Time: 80m 59s\n","Epoch valid loss: 2.818967187182641\n","Epoch EM: 57.32678299169913\n","Epoch F1: 69.2827315503881\n","====================================================================================\n"]}],"source":["\n","train_losses = []\n","valid_losses = []\n","ems = []\n","f1s = []\n","epochs = 3\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1}\")\n","    start_time = time.time()\n","    train_loss = train(model, train_dataset)\n","    valid_loss, em, f1 = valid(model, valid_dataset,'/kaggle/input/squad-dev/dev-v2.0.json')\n","    \n","    valid_loss = train_loss\n","    \n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","#     ems.append(em)\n","#     f1s.append(f1)\n","\n","    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"Epoch valid loss: {valid_loss}\")\n","    print(f\"Epoch EM: {em}\")\n","    print(f\"Epoch F1: {f1}\")\n","    print(\"====================================================================================\")\n","    "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T23:55:09.945121Z","iopub.status.busy":"2024-05-07T23:55:09.944817Z","iopub.status.idle":"2024-05-08T00:10:28.847068Z","shell.execute_reply":"2024-05-08T00:10:28.846138Z","shell.execute_reply.started":"2024-05-07T23:55:09.945094Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Evaluating .........\n","Validation Data is  57.32678299169913 69.2827315503881\n"]}],"source":["valid_loss, em, f1 = valid(model, valid_dataset, '/kaggle/input/squad-dev/dev-v2.0.json' )\n","print(\"Validating Data is \",em,f1)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T00:10:28.848663Z","iopub.status.busy":"2024-05-08T00:10:28.848380Z","iopub.status.idle":"2024-05-08T00:10:29.076217Z","shell.execute_reply":"2024-05-08T00:10:29.075309Z","shell.execute_reply.started":"2024-05-08T00:10:28.848636Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['when did beyonce became popular']\n","                          question                           context  \\\n","0  when did beyonce became popular  beyonce became popular in 1990s    \n","\n","     answer  id  \n","0  to find    1  \n","[1364, 118, 290, 6, 1442]\n","5 7\n","5 7\n","['beyonce', 'became', 'popular', 'in', '1990s']\n","torch.Size([1, 5]) torch.Size([1, 5])\n","(tensor([-2.3235, -1.6142, -1.0650,  4.4662,  8.0298], device='cuda:0'), tensor([-4.2915, -3.2854, -3.2325, -1.1319,  7.5230], device='cuda:0'))\n","tensor(4, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]}],"source":["import pandas as pd\n","def give_answer(df):\n","    context = df['context'].iloc[0]\n","    question = df['question'].iloc[0]\n","    context_ids = df['context_ids'].iloc[0]\n","    question_ids = df['question_ids'].iloc[0]\n","    \n","    print(context_ids)\n","    max_context_len = len(context_ids)\n","    max_question_len = len(question_ids)\n","    \n","    def make_char_vector( max_sent_len, max_word_len, sentence):\n","        \n","        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n","        \n","        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n","            for j, ch in enumerate(word.text):\n","                char_vec[i][j] = char2idx.get(ch, 0)\n","        \n","        return char_vec \n","    \n","    max_word_ctx = 0\n","    for word in nlp(context, disable=['parser','tagger','ner']):\n","        if len(word.text) > max_word_ctx:\n","            max_word_ctx = len(word.text)\n","                    \n","    print(max_context_len,max_word_ctx )\n","    char_ctx = torch.ones(1, max_context_len, max_word_ctx).type(torch.LongTensor)\n","#     for i, context_ in enumerate(context):\n","#         print(i,context_)\n","    char_ctx[0] = make_char_vector(max_context_len, max_word_ctx, context)\n","    \n","#     char_ques = torch.ones(1, max_question_len, max_word_ques).type(torch.LongTensor)\n","#     for i, question in enumerate(batch.question):\n","    max_word_ques=0\n","\n","    for word in nlp(question, disable=['parser','tagger','ner']):\n","        if len(word.text) > max_word_ques:\n","            max_word_ques = len(word.text)\n","            \n","            \n","    char_ques = torch.ones(1, max_question_len, max_word_ques).type(torch.LongTensor)\n","  \n","    print(max_question_len, max_word_ques)\n","    char_ques[0] = make_char_vector(max_question_len, max_word_ques, question)\n","    print(context.split())\n","    temp = context.split()\n","    context= torch.tensor(context_ids)\n","    question = torch.tensor(question_ids)\n","\n","    context, question, char_ctx, char_ques= context.to(device), question.to(device),\\\n","                                   char_ctx.to(device), char_ques.to(device)\n","    with torch.no_grad():\n","        context = torch.unsqueeze(context,0)\n","        question = torch.unsqueeze(question, 0)\n","        print(context.shape, question.shape)\n","        preds = model(context, question, char_ctx, char_ques)\n","        print(preds)\n","        p1,p2 = preds\n","        return p1,p2\n","        batch_size = 1\n","        for i in range(batch_size):\n","            id = ids[i]\n","            pred = context[i][s_idx[i]:e_idx[i]+1]\n","            pred = ' '.join([idx2word[idx.item()] for idx in pred])\n","            predictions[id] = pred\n","\n","# df = pd.DataFrame()\n","question = 'when did beyonce became popular'\n","context = 'beyonce became popular in 1990s '\n","# df['question'] = question\n","# df['context'] = context\n","print([question])\n","df = pd.DataFrame({\"question\":[question], \"context\":[context], \"answer\":['to find '], \"id\": [1]})\n","print(df.head())\n","\n","df['context_ids'] = df.context.apply(context_to_ids, word2idx=word2idx)\n","df['question_ids'] = df.question.apply(question_to_ids, word2idx=word2idx)\n","# train_dataset = SquadDataset(df, 1)\n","start,end = give_answer(df)\n","start = torch.argmax(start)\n","end = torch.argmax(end)\n","print(start)\n","# give_answer(context, question)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T00:10:29.077789Z","iopub.status.busy":"2024-05-08T00:10:29.077499Z","iopub.status.idle":"2024-05-08T00:10:29.282446Z","shell.execute_reply":"2024-05-08T00:10:29.281114Z","shell.execute_reply.started":"2024-05-08T00:10:29.077764Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":715814,"sourceId":1246668,"sourceType":"datasetVersion"},{"datasetId":4955708,"sourceId":8343262,"sourceType":"datasetVersion"},{"datasetId":4955731,"sourceId":8343290,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
