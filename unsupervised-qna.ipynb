{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6462034,"sourceType":"datasetVersion","datasetId":3731859}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"squad\")\nraw_datasets = raw_datasets.remove_columns([\"id\", \"title\"])\n\ndef prepare_data(example):\n    answer = example[\"answers\"][\"text\"][0]\n    example[\"answer_start\"] = example[\"answers\"][\"answer_start\"][0]\n    example[\"answer_end\"] = example[\"answer_start\"] + len(answer)\n    return example\n\nraw_datasets = raw_datasets.map(prepare_data, remove_columns=[\"answers\"])\nraw_datasets[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:11:44.138273Z","iopub.execute_input":"2024-04-08T13:11:44.139327Z","iopub.status.idle":"2024-04-08T13:12:08.848672Z","shell.execute_reply.started":"2024-04-08T13:11:44.139288Z","shell.execute_reply":"2024-04-08T13:12:08.847414Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c386160cd58448d2811af7c5711854e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f43d03a1b2ec4b5e8035806578b422e1"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97644f37bee45ddaa6d880dd4f6dfbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2652f953aa4cfcb28c4f49cf8dc795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1def43115254401bb0fb9b229124e9c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84434387800490bacdd5f7518bdccc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73e183eb88a4656a27a88ef27dbd386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/87599 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b203afa998a44e2795ddce0c0fa35527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10570 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499c2c21310b4a5984135bd58f2a0acf"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_start', 'answer_end'],\n    num_rows: 87599\n})"},"metadata":{}}]},{"cell_type":"code","source":"## 1. printing data instance\n\nprint(f\"Context: {raw_datasets['train'][0]['context']}\")\nprint(f\"Question: {raw_datasets['train'][0]['question']}\")\nstart=raw_datasets['train'][0]['answer_start']\nend=raw_datasets['train'][0]['answer_end']\nprint(f\"Answer start: {raw_datasets['train'][0]['answer_start']}\")\nprint(f\"Answer end: {raw_datasets['train'][0]['answer_end']}\")\nprint(f\"\\nAnswer: {raw_datasets['train'][0]['context'][start:end]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:12:18.189980Z","iopub.execute_input":"2024-04-08T13:12:18.190435Z","iopub.status.idle":"2024-04-08T13:12:18.201273Z","shell.execute_reply.started":"2024-04-08T13:12:18.190400Z","shell.execute_reply":"2024-04-08T13:12:18.199624Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\nQuestion: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nAnswer start: 515\nAnswer end: 541\n\nAnswer: Saint Bernadette Soubirous\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Converting to pandas DataFrame\n\nimport pandas as pd\ndf = pd.DataFrame(raw_datasets['train'])\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:12:20.420379Z","iopub.execute_input":"2024-04-08T13:12:20.420799Z","iopub.status.idle":"2024-04-08T13:12:26.958123Z","shell.execute_reply.started":"2024-04-08T13:12:20.420767Z","shell.execute_reply":"2024-04-08T13:12:26.956484Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"                                             context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            question  answer_start  answer_end  \n0  To whom did the Virgin Mary allegedly appear i...           515         541  \n1  What is in front of the Notre Dame Main Building?           188         213  \n2  The Basilica of the Sacred heart at Notre Dame...           279         296  \n3                  What is the Grotto at Notre Dame?           381         420  \n4  What sits on top of the Main Building at Notre...            92         126  \n","output_type":"stream"}]},{"cell_type":"code","source":"## 3. Appending the sentence_ind to the dataframe\n\nanswer_sent_index = []\ncontext_sentences = []\n\nfor i, sample in df.iterrows():\n        \n    context = sample['context']\n    answer_start = sample['answer_start']\n    answer_end = sample['answer_end'] \n\n    # Split the context into sentences\n    sentences = context.split(\". \")\n    context_sentences.append(sentences)\n    \n    # Find the sentence containing the answer\n    sentence_index = -1\n    for j, sentence in enumerate(sentences):\n        if answer_start >= context.find(sentence) and answer_end <= context.find(sentence) + len(sentence):\n            sentence_index = j\n            break\n    \n    answer_sent_index.append(sentence_index)\n\ndf['answer_sent_index'] = answer_sent_index\ndf['context_sentences'] = context_sentences","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:12:26.960217Z","iopub.execute_input":"2024-04-08T13:12:26.960681Z","iopub.status.idle":"2024-04-08T13:12:33.733690Z","shell.execute_reply.started":"2024-04-08T13:12:26.960641Z","shell.execute_reply":"2024-04-08T13:12:33.732551Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:12:33.735684Z","iopub.execute_input":"2024-04-08T13:12:33.736141Z","iopub.status.idle":"2024-04-08T13:12:33.742898Z","shell.execute_reply.started":"2024-04-08T13:12:33.736084Z","shell.execute_reply":"2024-04-08T13:12:33.742167Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['context', 'question', 'answer_start', 'answer_end',\n       'answer_sent_index', 'context_sentences'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Unsupervised Learning","metadata":{}},{"cell_type":"markdown","source":"## Jaccard Similarity","metadata":{}},{"cell_type":"code","source":"import nltk\nimport numpy as np\n\ndef get_jaccard_prediction(df_squad):\n    \"\"\"\n    Identify the answer sentence as one that has the largest Jaccard overlap with the input question.\n    \n    args:\n        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n        \n    returns:\n        pd.DataFrame : the input dataframe with two additional columns, \"jaccard_prediction\" and \"jaccard_value\"\n    \"\"\"\n    jaccard_max = list()\n    y_hat = list()\n    for index, row in df_squad.iterrows():\n        q = set(nltk.tokenize.word_tokenize(row['question']))\n        jaccard_list = list()\n        for sent in row['context_sentences']:\n            sentence = set(nltk.tokenize.word_tokenize(sent))\n            if q != set() or sentence != set():\n                intersection = len(q.intersection(sentence))\n                union = (len(q) + len(sentence)) - intersection\n                jaccard = float(intersection) / union\n                jaccard_list.append(jaccard)\n            else:\n                jaccard = 1\n                jaccard_list.append(jaccard)\n        jaccard_max.append(max(jaccard_list))\n        y_hat.append(np.argmax(jaccard_list))\n        \n#     df_squad['jaccard_value'] = jaccard_max\n    df_squad['jaccard_prediction'] = y_hat\n    return df_squad","metadata":{"execution":{"iopub.status.busy":"2024-04-08T12:02:25.224462Z","iopub.execute_input":"2024-04-08T12:02:25.224877Z","iopub.status.idle":"2024-04-08T12:02:25.235909Z","shell.execute_reply.started":"2024-04-08T12:02:25.224843Z","shell.execute_reply":"2024-04-08T12:02:25.234264Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"jaccard_df = df.head(1000).copy()\njaccard_df = get_jaccard_prediction(jaccard_df)\njaccard_accuracy = (jaccard_df['jaccard_prediction'] == jaccard_df[\"answer_sent_index\"]).values.mean()\nprint(f\"Accuracy of Jaccard Prediction on sentence indices: {jaccard_accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T12:04:53.292957Z","iopub.execute_input":"2024-04-08T12:04:53.293381Z","iopub.status.idle":"2024-04-08T12:04:55.827825Z","shell.execute_reply.started":"2024-04-08T12:04:53.293342Z","shell.execute_reply":"2024-04-08T12:04:55.826498Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy of Jaccard Prediction on sentence indices: 63.2%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TF-IDF Vectors","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\nimport scipy as sp\nimport numpy as np\n\ndef get_tfidf_prediction(df_squad, tfidf_vectorizer):\n    \"\"\"\n    Identify the answer sentence as one whose TF-IDF representation has minimal distance to that of the question.\n    \n    args:\n        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n        tfidf_vectorizer (sklearn.feature_extraction.text.TfidfVectorizer) :\n            the TF-IDF model to transform questions and sentences\n        \n    returns:\n        pd.DataFrame : the input dataframe with two additional columns, \"tfidf_prediction\" and \"distance_value\"\n    \"\"\"\n    tfidf_max = list()\n    y_hat = list()\n    \n    for index, row in df_squad.iterrows():\n        \n        tfidf_question = tfidf_vectorizer.transform([row['question']])\n        tfidf_context = tfidf_vectorizer.transform(row['context_sentences'])\n        \n        tfidf_list = list()\n        for j in range(tfidf_context.shape[0]):\n            dist = np.linalg.norm(tfidf_question[0].toarray() - tfidf_context[j].toarray())\n            tfidf_list.append(dist)\n      \n        tfidf_max.append(min(tfidf_list))\n        y_hat.append(np.argmin(tfidf_list))\n        \n#     df_squad['tfidf_value'] = tfidf_max\n    df_squad['tfidf_prediction'] = y_hat\n    return df_squad","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:25:09.665423Z","iopub.execute_input":"2024-04-08T13:25:09.666508Z","iopub.status.idle":"2024-04-08T13:25:09.678750Z","shell.execute_reply.started":"2024-04-08T13:25:09.666462Z","shell.execute_reply":"2024-04-08T13:25:09.677689Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(\n    tokenizer = nltk.word_tokenize,\n    stop_words = stopwords.words('english'),\n    ngram_range = (1,2),\n    max_df = 1.0,\n    min_df = 10\n)\ntfidf_vectorizer.fit(df[\"context\"].unique())","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:16:27.804987Z","iopub.execute_input":"2024-04-08T13:16:27.805461Z","iopub.status.idle":"2024-04-08T13:16:52.406137Z","shell.execute_reply.started":"2024-04-08T13:16:27.805424Z","shell.execute_reply":"2024-04-08T13:16:52.404966Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n                            'itself', ...],\n                tokenizer=<function word_tokenize at 0x7d2eb2b5dbd0>)","text/html":"<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n                            &#x27;itself&#x27;, ...],\n                tokenizer=&lt;function word_tokenize at 0x7d2eb2b5dbd0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n                            &#x27;itself&#x27;, ...],\n                tokenizer=&lt;function word_tokenize at 0x7d2eb2b5dbd0&gt;)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"abc = tfidf_question - tfidf_context[j]\nprint(type(abc))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:21:58.935633Z","iopub.execute_input":"2024-04-08T13:21:58.936051Z","iopub.status.idle":"2024-04-08T13:21:58.946040Z","shell.execute_reply.started":"2024-04-08T13:21:58.936009Z","shell.execute_reply":"2024-04-08T13:21:58.944574Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"<class 'scipy.sparse._csr.csr_matrix'>\n","output_type":"stream"}]},{"cell_type":"code","source":"tfidf_df = df.head(1000).copy()\ntfidf_df = get_tfidf_prediction(tfidf_df, tfidf_vectorizer)\ntfidf_accuracy = (tfidf_df['tfidf_prediction'] == tfidf_df[\"answer_sent_index\"]).values.mean()\nprint(f\"Accuracy of tf-idf Prediction on sentence indices: {tfidf_accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:25:16.969502Z","iopub.execute_input":"2024-04-08T13:25:16.970158Z","iopub.status.idle":"2024-04-08T13:25:27.313860Z","shell.execute_reply.started":"2024-04-08T13:25:16.970116Z","shell.execute_reply":"2024-04-08T13:25:27.312268Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Accuracy of tf-idf Prediction on sentence indices: 58.8%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sent2vec Encoders","metadata":{}},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:56:43.255669Z","iopub.execute_input":"2024-04-08T11:56:43.256115Z","iopub.status.idle":"2024-04-08T11:56:59.966043Z","shell.execute_reply.started":"2024-04-08T11:56:43.256082Z","shell.execute_reply":"2024-04-08T11:56:59.964768Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.21.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\na = torch.tensor([[10, 11, 12]])\np, q = torch.max(a, dim = 1)\nprint(p, q)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:57:15.078862Z","iopub.execute_input":"2024-04-08T11:57:15.079299Z","iopub.status.idle":"2024-04-08T11:57:19.121952Z","shell.execute_reply.started":"2024-04-08T11:57:15.079265Z","shell.execute_reply":"2024-04-08T11:57:19.120605Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([12]) tensor([2])\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_sent2vec_prediction(df_squad, encoder):\n    \"\"\"\n    Identify the answer sentence as one that has the max cosine similarity with the input question.\n    \n    args:\n        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n        \n    returns:\n        pd.DataFrame : the input dataframe with two additional columns, \"sent2vec_prediction\" and \"sent2vec_value\"\n    \"\"\"\n    sen2vec_max = list()\n    y_hat = list()\n    \n    for index, row in df_squad.iterrows():\n        question_embedding = encoder.encode([row['question']], convert_to_tensor=True)\n        context_embedding = encoder.encode(row['context_sentences'], convert_to_tensor=True)\n        \n        cosine_scores = util.cos_sim(question_embedding, context_embedding)\n        max_score, max_ind = torch.max(cosine_scores, dim = 1)\n        sen2vec_max.append(max_score[0])\n        y_hat.append(max_ind[0])\n        \n        if index % 1000 == 0: print(index)\n        \n#     df_squad['sen2vec_value'] = sen2vec_max\n    df_squad['sent2vec_prediction'] = y_hat\n    return df_squad","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:57:22.048325Z","iopub.execute_input":"2024-04-08T11:57:22.049306Z","iopub.status.idle":"2024-04-08T11:57:22.058090Z","shell.execute_reply.started":"2024-04-08T11:57:22.049267Z","shell.execute_reply":"2024-04-08T11:57:22.056748Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## 1. Defining the encoder\n\nfrom sentence_transformers import SentenceTransformer, util\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 2. Making the prediction\n\nsent2vec_df = df.head(1000).copy()\nsent2vec_df = get_sent2vec_prediction(sent2vec_df, encoder)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 3. printing the accuracy\n\nsent2vec_accuracy = (sent2vec_df['sent2vec_prediction'] == sent2vec_df[\"answer_sent_index\"]).values.mean()\nprint(f\"Accuracy of sent2vec Prediction on sentence indices: {sent2vec_accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T12:01:09.744120Z","iopub.execute_input":"2024-04-08T12:01:09.744500Z","iopub.status.idle":"2024-04-08T12:01:09.760397Z","shell.execute_reply.started":"2024-04-08T12:01:09.744470Z","shell.execute_reply":"2024-04-08T12:01:09.758933Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy of sent2vec Prediction on sentence indices: 66.60000000000001%\n","output_type":"stream"}]}]}